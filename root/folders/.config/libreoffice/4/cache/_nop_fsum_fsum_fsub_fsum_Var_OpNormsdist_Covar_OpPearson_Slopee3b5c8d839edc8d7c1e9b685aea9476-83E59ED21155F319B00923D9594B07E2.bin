//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27597688
// Unknown Toolkit Version
// Based on LLVM 3.4svn
//

.version 6.5
.target sm_61, texmode_independent
.address_size 64

	// .globl	DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope

.entry DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope(
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9
)
{
	.reg .pred 	%p<261>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<279>;
	.reg .f64 	%fd<1478>;
	.reg .b64 	%rd<73>;


	ld.param.u64 	%rd9, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4];
	ld.param.u64 	%rd10, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5];
	ld.param.u64 	%rd11, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6];
	ld.param.u64 	%rd12, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7];
	ld.param.u64 	%rd13, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8];
	ld.param.u64 	%rd14, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9];
	mov.b32	%r111, %envreg3;
	mov.u32 	%r112, %ctaid.x;
	mov.u32 	%r113, %ntid.x;
	mad.lo.s32 	%r1, %r112, %r113, %r111;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	mov.f64 	%fd1402, 0d0000000000000000;
	setp.gt.s32	%p1, %r3, 5;
	mov.f64 	%fd1401, %fd1402;
	mov.f64 	%fd1400, %fd1402;
	@%p1 bra 	BB0_6;

	add.s32 	%r245, %r1, %r2;
	mov.f64 	%fd1402, 0d0000000000000000;
	mov.u32 	%r244, 1;
	mov.f64 	%fd1401, %fd1402;
	mov.f64 	%fd1400, %fd1402;

BB0_2:
	mul.wide.s32 	%rd15, %r245, 8;
	add.s64 	%rd16, %rd14, %rd15;
	add.s64 	%rd17, %rd13, %rd15;
	ld.global.f64 	%fd4, [%rd17];
	ld.global.f64 	%fd5, [%rd16];
	abs.f64 	%fd239, %fd5;
	setp.gtu.f64	%p2, %fd239, 0d7FF0000000000000;
	@%p2 bra 	BB0_5;

	abs.f64 	%fd240, %fd4;
	setp.gtu.f64	%p3, %fd240, 0d7FF0000000000000;
	@%p3 bra 	BB0_5;

	add.f64 	%fd1402, %fd1402, %fd5;
	add.f64 	%fd1401, %fd1401, %fd4;
	add.f64 	%fd1400, %fd1400, 0d3FF0000000000000;

BB0_5:
	add.s32 	%r245, %r245, 1;
	setp.lt.s32	%p4, %r245, 6;
	setp.lt.s32	%p5, %r244, 2;
	and.pred  	%p6, %p4, %p5;
	add.s32 	%r244, %r244, 1;
	@%p6 bra 	BB0_2;

BB0_6:
	setp.lt.f64	%p7, %fd1400, 0d3FF0000000000000;
	mov.f64 	%fd1422, 0d7FF8000000000207;
	@%p7 bra 	BB0_65;

	setp.eq.f64	%p8, %fd1400, 0d3FF0000000000000;
	mov.f64 	%fd1410, 0d3FF0000000000000;
	@%p8 bra 	BB0_33;

	abs.f64 	%fd15, %fd1400;
	setp.gtu.f64	%p9, %fd15, 0d7FF0000000000000;
	@%p9 bra 	BB0_32;
	bra.uni 	BB0_9;

BB0_32:
	add.f64 	%fd1410, %fd1400, 0dBFF0000000000000;
	bra.uni 	BB0_33;

BB0_9:
	setp.eq.f64	%p10, %fd1400, 0d7FF0000000000000;
	@%p10 bra 	BB0_31;
	bra.uni 	BB0_10;

BB0_31:
	mov.f64 	%fd433, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r135}, %fd433;
	}
	setp.gt.s32	%p27, %r135, -1;
	selp.f64	%fd1410, 0d7FF0000000000000, 0d0000000000000000, %p27;
	bra.uni 	BB0_33;

BB0_10:
	mov.f64 	%fd243, 0dBFF0000000000000;
	mov.f64 	%fd244, 0d3FE0000000000000;
	mul.rn.f64 	%fd245, %fd244, %fd243;
	cvt.rzi.f64.f64	%fd246, %fd245;
	mov.f64 	%fd247, 0d4000000000000000;
	mul.rn.f64 	%fd248, %fd247, %fd246;
	sub.f64 	%fd249, %fd243, %fd248;
	abs.f64 	%fd16, %fd249;
	setp.eq.f64	%p11, %fd1400, 0d0000000000000000;
	@%p11 bra 	BB0_30;
	bra.uni 	BB0_11;

BB0_30:
	setp.eq.f64	%p26, %fd16, 0d3FF0000000000000;
	rcp.rn.f64 	%fd430, %fd1400;
	mov.f64 	%fd431, 0d0000000000000000;
	rcp.rn.f64 	%fd432, %fd431;
	selp.f64	%fd1410, %fd430, %fd432, %p26;
	bra.uni 	BB0_33;

BB0_11:
	setp.eq.f64	%p12, %fd1400, 0dFFF0000000000000;
	@%p12 bra 	BB0_28;
	bra.uni 	BB0_12;

BB0_28:
	div.rn.f64 	%fd1410, %fd243, %fd1400;
	setp.neu.f64	%p25, %fd16, 0d3FF0000000000000;
	@%p25 bra 	BB0_33;

	mov.b64 	 %rd20, %fd1410;
	xor.b64  	%rd21, %rd20, -9223372036854775808;
	mov.b64 	 %fd1410, %rd21;
	bra.uni 	BB0_33;

BB0_12:
	setp.geu.f64	%p13, %fd1400, 0d0000000000000000;
	@%p13 bra 	BB0_14;

	cvt.rzi.f64.f64	%fd252, %fd243;
	setp.neu.f64	%p14, %fd252, 0dBFF0000000000000;
	mov.f64 	%fd1410, 0dFFF8000000000000;
	@%p14 bra 	BB0_33;

BB0_14:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r247}, %fd15; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r246, hi}, %fd15; 
	}
	// inline asm
	bfe.u32 	%r248, %r247, 20, 11;
	setp.ne.s32	%p15, %r248, 0;
	@%p15 bra 	BB0_16;

	mov.f64 	%fd257, 0d4350000000000000;
	mul.rn.f64 	%fd256, %fd15, %fd257;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r247}, %fd256; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r246, hi}, %fd256; 
	}
	// inline asm
	bfe.u32 	%r119, %r247, 20, 11;
	add.s32 	%r248, %r119, -54;

BB0_16:
	and.b32  	%r122, %r247, -2146435073;
	or.b32  	%r121, %r122, 1072693248;
	// inline asm
	mov.b64 	%fd1406, {%r246, %r121};
	// inline asm
	add.s32 	%r249, %r248, -1023;
	setp.lt.u32	%p16, %r121, 1073127583;
	@%p16 bra 	BB0_18;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r123, hi}, %fd1406; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r124}, %fd1406; 
	}
	// inline asm
	add.s32 	%r126, %r124, -1048576;
	// inline asm
	mov.b64 	%fd1406, {%r123, %r126};
	// inline asm
	add.s32 	%r249, %r248, -1022;

BB0_18:
	add.f64 	%fd346, %fd1406, 0d3FF0000000000000;
	rcp.rn.f64 	%fd347, %fd346;
	add.f64 	%fd288, %fd1406, 0dBFF0000000000000;
	mul.rn.f64 	%fd348, %fd288, %fd347;
	add.f64 	%fd336, %fd348, %fd348;
	mul.rn.f64 	%fd284, %fd336, %fd336;
	mov.f64 	%fd263, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd265, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd262, %fd263, %fd284, %fd265;
	// inline asm
	mov.f64 	%fd269, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd266, %fd262, %fd284, %fd269;
	// inline asm
	mov.f64 	%fd273, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd270, %fd266, %fd284, %fd273;
	// inline asm
	mov.f64 	%fd277, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd274, %fd270, %fd284, %fd277;
	// inline asm
	mov.f64 	%fd281, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd278, %fd274, %fd284, %fd281;
	// inline asm
	mov.f64 	%fd285, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd282, %fd278, %fd284, %fd285;
	// inline asm
	mul.rn.f64 	%fd349, %fd282, %fd284;
	sub.f64 	%fd350, %fd288, %fd336;
	mul.rn.f64 	%fd289, %fd247, %fd350;
	neg.f64 	%fd287, %fd336;
	// inline asm
	fma.rn.f64 	%fd286, %fd287, %fd288, %fd289;
	// inline asm
	mul.rn.f64 	%fd332, %fd347, %fd286;
	add.f64 	%fd352, %fd349, 0d3FB5555555555555;
	mov.f64 	%fd353, 0d3FB5555555555555;
	sub.f64 	%fd354, %fd353, %fd352;
	add.f64 	%fd355, %fd349, %fd354;
	add.f64 	%fd356, %fd355, 0d0000000000000000;
	add.f64 	%fd357, %fd356, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd299, %fd352, %fd357;
	sub.f64 	%fd358, %fd352, %fd299;
	add.f64 	%fd303, %fd357, %fd358;
	mul.rn.f64 	%fd359, %fd299, %fd336;
	neg.f64 	%fd293, %fd359;
	// inline asm
	fma.rn.f64 	%fd290, %fd299, %fd336, %fd293;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd294, %fd303, %fd332, %fd290;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd298, %fd299, %fd332, %fd294;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd302, %fd303, %fd336, %fd298;
	// inline asm
	add.f64 	%fd315, %fd359, %fd302;
	sub.f64 	%fd360, %fd359, %fd315;
	add.f64 	%fd319, %fd302, %fd360;
	mul.rn.f64 	%fd361, %fd315, %fd336;
	neg.f64 	%fd309, %fd361;
	// inline asm
	fma.rn.f64 	%fd306, %fd315, %fd336, %fd309;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd310, %fd319, %fd332, %fd306;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd314, %fd315, %fd332, %fd310;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd318, %fd319, %fd336, %fd314;
	// inline asm
	add.f64 	%fd331, %fd361, %fd318;
	sub.f64 	%fd362, %fd361, %fd331;
	add.f64 	%fd335, %fd318, %fd362;
	mul.rn.f64 	%fd363, %fd331, %fd336;
	neg.f64 	%fd325, %fd363;
	// inline asm
	fma.rn.f64 	%fd322, %fd331, %fd336, %fd325;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd326, %fd335, %fd332, %fd322;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd330, %fd331, %fd332, %fd326;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd334, %fd335, %fd336, %fd330;
	// inline asm
	add.f64 	%fd364, %fd363, %fd334;
	sub.f64 	%fd365, %fd363, %fd364;
	add.f64 	%fd366, %fd334, %fd365;
	add.f64 	%fd367, %fd336, %fd364;
	sub.f64 	%fd368, %fd336, %fd367;
	add.f64 	%fd369, %fd364, %fd368;
	add.f64 	%fd370, %fd366, %fd369;
	add.f64 	%fd371, %fd332, %fd370;
	add.f64 	%fd372, %fd367, %fd371;
	sub.f64 	%fd373, %fd367, %fd372;
	add.f64 	%fd374, %fd371, %fd373;
	cvt.rn.f64.s32	%fd375, %r249;
	mov.f64 	%fd376, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd377, %fd375, %fd376;
	mov.f64 	%fd378, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd379, %fd375, %fd378;
	add.f64 	%fd380, %fd377, %fd372;
	sub.f64 	%fd381, %fd377, %fd380;
	add.f64 	%fd382, %fd372, %fd381;
	add.f64 	%fd383, %fd374, %fd382;
	add.f64 	%fd384, %fd379, %fd383;
	add.f64 	%fd339, %fd380, %fd384;
	sub.f64 	%fd385, %fd380, %fd339;
	add.f64 	%fd343, %fd384, %fd385;
	mul.rn.f64 	%fd386, %fd339, %fd243;
	neg.f64 	%fd341, %fd386;
	// inline asm
	fma.rn.f64 	%fd338, %fd339, %fd243, %fd341;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd342, %fd343, %fd243, %fd338;
	// inline asm
	add.f64 	%fd20, %fd386, %fd342;
	sub.f64 	%fd387, %fd386, %fd20;
	add.f64 	%fd21, %fd342, %fd387;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd20;
	}
	mov.b32 	 %f1, %r21;
	abs.f32 	%f2, %f1;
	setp.lt.f32	%p17, %f2, 0f40874911;
	@%p17 bra 	BB0_20;
	bra.uni 	BB0_19;

BB0_20:
	mov.f64 	%fd391, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd392, %fd20, %fd391;
	mov.f64 	%fd393, 0d4338000000000000;
	add.rn.f64 	%fd394, %fd392, %fd393;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd394;
	}
	mov.f64 	%fd395, 0dC338000000000000;
	add.rn.f64 	%fd396, %fd394, %fd395;
	mov.f64 	%fd397, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd398, %fd396, %fd397, %fd20;
	mov.f64 	%fd399, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd400, %fd396, %fd399, %fd398;
	mov.f64 	%fd401, 0d3E928AF3FCA213EA;
	mov.f64 	%fd402, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd403, %fd402, %fd400, %fd401;
	mov.f64 	%fd404, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd405, %fd403, %fd400, %fd404;
	mov.f64 	%fd406, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd407, %fd405, %fd400, %fd406;
	mov.f64 	%fd408, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd409, %fd407, %fd400, %fd408;
	mov.f64 	%fd410, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd411, %fd409, %fd400, %fd410;
	mov.f64 	%fd412, 0d3F81111111122322;
	fma.rn.f64 	%fd413, %fd411, %fd400, %fd412;
	mov.f64 	%fd414, 0d3FA55555555502A1;
	fma.rn.f64 	%fd415, %fd413, %fd400, %fd414;
	mov.f64 	%fd416, 0d3FC5555555555511;
	fma.rn.f64 	%fd417, %fd415, %fd400, %fd416;
	mov.f64 	%fd418, 0d3FE000000000000B;
	fma.rn.f64 	%fd419, %fd417, %fd400, %fd418;
	mov.f64 	%fd420, 0d3FF0000000000000;
	fma.rn.f64 	%fd421, %fd419, %fd400, %fd420;
	fma.rn.f64 	%fd1407, %fd421, %fd400, %fd420;
	abs.s32 	%r127, %r22;
	setp.lt.s32	%p20, %r127, 1023;
	@%p20 bra 	BB0_22;
	bra.uni 	BB0_21;

BB0_22:
	shl.b32 	%r133, %r22, 20;
	add.s32 	%r250, %r133, 1072693248;
	bra.uni 	BB0_23;

BB0_19:
	setp.lt.s32	%p18, %r21, 0;
	selp.f64	%fd388, 0d0000000000000000, 0d7FF0000000000000, %p18;
	abs.f64 	%fd389, %fd20;
	setp.gtu.f64	%p19, %fd389, 0d7FF0000000000000;
	add.f64 	%fd390, %fd20, %fd20;
	selp.f64	%fd1410, %fd390, %fd388, %p19;
	bra.uni 	BB0_24;

BB0_21:
	add.s32 	%r128, %r22, 2046;
	shl.b32 	%r129, %r128, 19;
	and.b32  	%r130, %r129, -1048576;
	shl.b32 	%r131, %r128, 20;
	sub.s32 	%r250, %r131, %r130;
	mov.u32 	%r132, 0;
	mov.b64 	%fd422, {%r132, %r130};
	mul.f64 	%fd1407, %fd1407, %fd422;

BB0_23:
	mov.u32 	%r134, 0;
	mov.b64 	%fd423, {%r134, %r250};
	mul.f64 	%fd1410, %fd1407, %fd423;

BB0_24:
	abs.f64 	%fd424, %fd1410;
	setp.eq.f64	%p21, %fd424, 0d7FF0000000000000;
	@%p21 bra 	BB0_26;

	// inline asm
	fma.rn.f64 	%fd1410, %fd1410, %fd21, %fd1410;
	// inline asm

BB0_26:
	setp.neu.f64	%p22, %fd16, 0d3FF0000000000000;
	or.pred  	%p24, %p13, %p22;
	@%p24 bra 	BB0_33;

	mov.b64 	 %rd18, %fd1410;
	xor.b64  	%rd19, %rd18, -9223372036854775808;
	mov.b64 	 %fd1410, %rd19;

BB0_33:
	mul.f64 	%fd37, %fd1402, %fd1410;
	mul.f64 	%fd38, %fd1401, %fd1410;
	mov.f64 	%fd1414, 0d0000000000000000;
	mov.f64 	%fd1413, %fd1414;
	@%p1 bra 	BB0_39;

	mov.f64 	%fd1414, 0d0000000000000000;
	mov.u32 	%r251, 1;
	mov.u32 	%r252, %r3;
	mov.f64 	%fd1413, %fd1414;

BB0_35:
	mul.wide.s32 	%rd22, %r252, 8;
	add.s64 	%rd23, %rd14, %rd22;
	add.s64 	%rd24, %rd13, %rd22;
	ld.global.f64 	%fd41, [%rd24];
	ld.global.f64 	%fd42, [%rd23];
	abs.f64 	%fd438, %fd42;
	setp.gtu.f64	%p29, %fd438, 0d7FF0000000000000;
	@%p29 bra 	BB0_38;

	abs.f64 	%fd439, %fd41;
	setp.gtu.f64	%p30, %fd439, 0d7FF0000000000000;
	@%p30 bra 	BB0_38;

	sub.f64 	%fd440, %fd42, %fd37;
	sub.f64 	%fd441, %fd41, %fd38;
	fma.rn.f64 	%fd1414, %fd440, %fd441, %fd1414;
	fma.rn.f64 	%fd1413, %fd440, %fd440, %fd1413;

BB0_38:
	add.s32 	%r252, %r252, 1;
	setp.lt.s32	%p31, %r252, 6;
	setp.lt.s32	%p32, %r251, 2;
	and.pred  	%p33, %p31, %p32;
	add.s32 	%r251, %r251, 1;
	@%p33 bra 	BB0_35;

BB0_39:
	setp.eq.f64	%p34, %fd1413, 0d0000000000000000;
	mov.f64 	%fd1422, 0d7FF8000000000214;
	@%p34 bra 	BB0_65;

	setp.eq.f64	%p35, %fd1413, 0d3FF0000000000000;
	mov.f64 	%fd1421, 0d3FF0000000000000;
	@%p35 bra 	BB0_64;

	abs.f64 	%fd49, %fd1413;
	setp.gtu.f64	%p36, %fd49, 0d7FF0000000000000;
	@%p36 bra 	BB0_63;
	bra.uni 	BB0_42;

BB0_63:
	add.f64 	%fd1421, %fd1413, 0dBFF0000000000000;
	bra.uni 	BB0_64;

BB0_42:
	setp.eq.f64	%p37, %fd1413, 0d7FF0000000000000;
	@%p37 bra 	BB0_62;
	bra.uni 	BB0_43;

BB0_62:
	mov.f64 	%fd631, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r158}, %fd631;
	}
	setp.gt.s32	%p52, %r158, -1;
	selp.f64	%fd1421, 0d7FF0000000000000, 0d0000000000000000, %p52;
	bra.uni 	BB0_64;

BB0_43:
	mov.f64 	%fd444, 0dBFF0000000000000;
	mov.f64 	%fd445, 0d3FE0000000000000;
	mul.rn.f64 	%fd446, %fd445, %fd444;
	cvt.rzi.f64.f64	%fd447, %fd446;
	mov.f64 	%fd448, 0d4000000000000000;
	mul.rn.f64 	%fd449, %fd448, %fd447;
	sub.f64 	%fd450, %fd444, %fd449;
	abs.f64 	%fd50, %fd450;
	setp.eq.f64	%p38, %fd1413, 0dFFF0000000000000;
	@%p38 bra 	BB0_60;
	bra.uni 	BB0_44;

BB0_60:
	div.rn.f64 	%fd1421, %fd444, %fd1413;
	setp.neu.f64	%p51, %fd50, 0d3FF0000000000000;
	@%p51 bra 	BB0_64;

	mov.b64 	 %rd27, %fd1421;
	xor.b64  	%rd28, %rd27, -9223372036854775808;
	mov.b64 	 %fd1421, %rd28;
	bra.uni 	BB0_64;

BB0_44:
	setp.geu.f64	%p39, %fd1413, 0d0000000000000000;
	@%p39 bra 	BB0_46;

	cvt.rzi.f64.f64	%fd453, %fd444;
	setp.neu.f64	%p40, %fd453, 0dBFF0000000000000;
	mov.f64 	%fd1421, 0dFFF8000000000000;
	@%p40 bra 	BB0_64;

BB0_46:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r254}, %fd49; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r253, hi}, %fd49; 
	}
	// inline asm
	bfe.u32 	%r255, %r254, 20, 11;
	setp.ne.s32	%p41, %r255, 0;
	@%p41 bra 	BB0_48;

	mov.f64 	%fd458, 0d4350000000000000;
	mul.rn.f64 	%fd457, %fd49, %fd458;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r254}, %fd457; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r253, hi}, %fd457; 
	}
	// inline asm
	bfe.u32 	%r142, %r254, 20, 11;
	add.s32 	%r255, %r142, -54;

BB0_48:
	and.b32  	%r145, %r254, -2146435073;
	or.b32  	%r144, %r145, 1072693248;
	// inline asm
	mov.b64 	%fd1417, {%r253, %r144};
	// inline asm
	add.s32 	%r256, %r255, -1023;
	setp.lt.u32	%p42, %r144, 1073127583;
	@%p42 bra 	BB0_50;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r146, hi}, %fd1417; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r147}, %fd1417; 
	}
	// inline asm
	add.s32 	%r149, %r147, -1048576;
	// inline asm
	mov.b64 	%fd1417, {%r146, %r149};
	// inline asm
	add.s32 	%r256, %r255, -1022;

BB0_50:
	add.f64 	%fd547, %fd1417, 0d3FF0000000000000;
	rcp.rn.f64 	%fd548, %fd547;
	add.f64 	%fd489, %fd1417, 0dBFF0000000000000;
	mul.rn.f64 	%fd549, %fd489, %fd548;
	add.f64 	%fd537, %fd549, %fd549;
	mul.rn.f64 	%fd485, %fd537, %fd537;
	mov.f64 	%fd464, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd466, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd463, %fd464, %fd485, %fd466;
	// inline asm
	mov.f64 	%fd470, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd467, %fd463, %fd485, %fd470;
	// inline asm
	mov.f64 	%fd474, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd471, %fd467, %fd485, %fd474;
	// inline asm
	mov.f64 	%fd478, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd475, %fd471, %fd485, %fd478;
	// inline asm
	mov.f64 	%fd482, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd479, %fd475, %fd485, %fd482;
	// inline asm
	mov.f64 	%fd486, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd483, %fd479, %fd485, %fd486;
	// inline asm
	mul.rn.f64 	%fd550, %fd483, %fd485;
	sub.f64 	%fd551, %fd489, %fd537;
	mul.rn.f64 	%fd490, %fd448, %fd551;
	neg.f64 	%fd488, %fd537;
	// inline asm
	fma.rn.f64 	%fd487, %fd488, %fd489, %fd490;
	// inline asm
	mul.rn.f64 	%fd533, %fd548, %fd487;
	add.f64 	%fd553, %fd550, 0d3FB5555555555555;
	mov.f64 	%fd554, 0d3FB5555555555555;
	sub.f64 	%fd555, %fd554, %fd553;
	add.f64 	%fd556, %fd550, %fd555;
	add.f64 	%fd557, %fd556, 0d0000000000000000;
	add.f64 	%fd558, %fd557, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd500, %fd553, %fd558;
	sub.f64 	%fd559, %fd553, %fd500;
	add.f64 	%fd504, %fd558, %fd559;
	mul.rn.f64 	%fd560, %fd500, %fd537;
	neg.f64 	%fd494, %fd560;
	// inline asm
	fma.rn.f64 	%fd491, %fd500, %fd537, %fd494;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd495, %fd504, %fd533, %fd491;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd499, %fd500, %fd533, %fd495;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd503, %fd504, %fd537, %fd499;
	// inline asm
	add.f64 	%fd516, %fd560, %fd503;
	sub.f64 	%fd561, %fd560, %fd516;
	add.f64 	%fd520, %fd503, %fd561;
	mul.rn.f64 	%fd562, %fd516, %fd537;
	neg.f64 	%fd510, %fd562;
	// inline asm
	fma.rn.f64 	%fd507, %fd516, %fd537, %fd510;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd511, %fd520, %fd533, %fd507;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd515, %fd516, %fd533, %fd511;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd519, %fd520, %fd537, %fd515;
	// inline asm
	add.f64 	%fd532, %fd562, %fd519;
	sub.f64 	%fd563, %fd562, %fd532;
	add.f64 	%fd536, %fd519, %fd563;
	mul.rn.f64 	%fd564, %fd532, %fd537;
	neg.f64 	%fd526, %fd564;
	// inline asm
	fma.rn.f64 	%fd523, %fd532, %fd537, %fd526;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd527, %fd536, %fd533, %fd523;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd531, %fd532, %fd533, %fd527;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd535, %fd536, %fd537, %fd531;
	// inline asm
	add.f64 	%fd565, %fd564, %fd535;
	sub.f64 	%fd566, %fd564, %fd565;
	add.f64 	%fd567, %fd535, %fd566;
	add.f64 	%fd568, %fd537, %fd565;
	sub.f64 	%fd569, %fd537, %fd568;
	add.f64 	%fd570, %fd565, %fd569;
	add.f64 	%fd571, %fd567, %fd570;
	add.f64 	%fd572, %fd533, %fd571;
	add.f64 	%fd573, %fd568, %fd572;
	sub.f64 	%fd574, %fd568, %fd573;
	add.f64 	%fd575, %fd572, %fd574;
	cvt.rn.f64.s32	%fd576, %r256;
	mov.f64 	%fd577, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd578, %fd576, %fd577;
	mov.f64 	%fd579, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd580, %fd576, %fd579;
	add.f64 	%fd581, %fd578, %fd573;
	sub.f64 	%fd582, %fd578, %fd581;
	add.f64 	%fd583, %fd573, %fd582;
	add.f64 	%fd584, %fd575, %fd583;
	add.f64 	%fd585, %fd580, %fd584;
	add.f64 	%fd540, %fd581, %fd585;
	sub.f64 	%fd586, %fd581, %fd540;
	add.f64 	%fd544, %fd585, %fd586;
	mul.rn.f64 	%fd587, %fd540, %fd444;
	neg.f64 	%fd542, %fd587;
	// inline asm
	fma.rn.f64 	%fd539, %fd540, %fd444, %fd542;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd543, %fd544, %fd444, %fd539;
	// inline asm
	add.f64 	%fd54, %fd587, %fd543;
	sub.f64 	%fd588, %fd587, %fd54;
	add.f64 	%fd55, %fd543, %fd588;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd54;
	}
	mov.b32 	 %f3, %r43;
	abs.f32 	%f4, %f3;
	setp.lt.f32	%p43, %f4, 0f40874911;
	@%p43 bra 	BB0_52;
	bra.uni 	BB0_51;

BB0_52:
	mov.f64 	%fd592, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd593, %fd54, %fd592;
	mov.f64 	%fd594, 0d4338000000000000;
	add.rn.f64 	%fd595, %fd593, %fd594;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r44, %temp}, %fd595;
	}
	mov.f64 	%fd596, 0dC338000000000000;
	add.rn.f64 	%fd597, %fd595, %fd596;
	mov.f64 	%fd598, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd599, %fd597, %fd598, %fd54;
	mov.f64 	%fd600, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd601, %fd597, %fd600, %fd599;
	mov.f64 	%fd602, 0d3E928AF3FCA213EA;
	mov.f64 	%fd603, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd604, %fd603, %fd601, %fd602;
	mov.f64 	%fd605, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd606, %fd604, %fd601, %fd605;
	mov.f64 	%fd607, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd608, %fd606, %fd601, %fd607;
	mov.f64 	%fd609, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd610, %fd608, %fd601, %fd609;
	mov.f64 	%fd611, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd612, %fd610, %fd601, %fd611;
	mov.f64 	%fd613, 0d3F81111111122322;
	fma.rn.f64 	%fd614, %fd612, %fd601, %fd613;
	mov.f64 	%fd615, 0d3FA55555555502A1;
	fma.rn.f64 	%fd616, %fd614, %fd601, %fd615;
	mov.f64 	%fd617, 0d3FC5555555555511;
	fma.rn.f64 	%fd618, %fd616, %fd601, %fd617;
	mov.f64 	%fd619, 0d3FE000000000000B;
	fma.rn.f64 	%fd620, %fd618, %fd601, %fd619;
	mov.f64 	%fd621, 0d3FF0000000000000;
	fma.rn.f64 	%fd622, %fd620, %fd601, %fd621;
	fma.rn.f64 	%fd1418, %fd622, %fd601, %fd621;
	abs.s32 	%r150, %r44;
	setp.lt.s32	%p46, %r150, 1023;
	@%p46 bra 	BB0_54;
	bra.uni 	BB0_53;

BB0_54:
	shl.b32 	%r156, %r44, 20;
	add.s32 	%r257, %r156, 1072693248;
	bra.uni 	BB0_55;

BB0_51:
	setp.lt.s32	%p44, %r43, 0;
	selp.f64	%fd589, 0d0000000000000000, 0d7FF0000000000000, %p44;
	abs.f64 	%fd590, %fd54;
	setp.gtu.f64	%p45, %fd590, 0d7FF0000000000000;
	add.f64 	%fd591, %fd54, %fd54;
	selp.f64	%fd1421, %fd591, %fd589, %p45;
	bra.uni 	BB0_56;

BB0_53:
	add.s32 	%r151, %r44, 2046;
	shl.b32 	%r152, %r151, 19;
	and.b32  	%r153, %r152, -1048576;
	shl.b32 	%r154, %r151, 20;
	sub.s32 	%r257, %r154, %r153;
	mov.u32 	%r155, 0;
	mov.b64 	%fd623, {%r155, %r153};
	mul.f64 	%fd1418, %fd1418, %fd623;

BB0_55:
	mov.u32 	%r157, 0;
	mov.b64 	%fd624, {%r157, %r257};
	mul.f64 	%fd1421, %fd1418, %fd624;

BB0_56:
	abs.f64 	%fd625, %fd1421;
	setp.eq.f64	%p47, %fd625, 0d7FF0000000000000;
	@%p47 bra 	BB0_58;

	// inline asm
	fma.rn.f64 	%fd1421, %fd1421, %fd55, %fd1421;
	// inline asm

BB0_58:
	setp.neu.f64	%p48, %fd50, 0d3FF0000000000000;
	or.pred  	%p50, %p39, %p48;
	@%p50 bra 	BB0_64;

	mov.b64 	 %rd25, %fd1421;
	xor.b64  	%rd26, %rd25, -9223372036854775808;
	mov.b64 	 %fd1421, %rd26;

BB0_64:
	mul.f64 	%fd1422, %fd1414, %fd1421;

BB0_65:
	add.f64 	%fd72, %fd1422, 0d0000000000000000;
	mov.f64 	%fd1428, 0d0000000000000000;
	mov.f64 	%fd1427, %fd1428;
	mov.f64 	%fd1426, %fd1428;
	@%p1 bra 	BB0_71;

	mov.u32 	%r243, %tid.x;
	add.s32 	%r259, %r1, %r243;
	mov.f64 	%fd1428, 0d0000000000000000;
	mov.u32 	%r258, 1;
	mov.f64 	%fd1427, %fd1428;
	mov.f64 	%fd1426, %fd1428;

BB0_67:
	mul.wide.s32 	%rd29, %r259, 8;
	add.s64 	%rd30, %rd11, %rd29;
	add.s64 	%rd31, %rd12, %rd29;
	ld.global.f64 	%fd76, [%rd31];
	ld.global.f64 	%fd77, [%rd30];
	abs.f64 	%fd638, %fd77;
	setp.gtu.f64	%p54, %fd638, 0d7FF0000000000000;
	@%p54 bra 	BB0_70;

	abs.f64 	%fd639, %fd76;
	setp.gtu.f64	%p55, %fd639, 0d7FF0000000000000;
	@%p55 bra 	BB0_70;

	add.f64 	%fd1427, %fd1427, %fd77;
	add.f64 	%fd1426, %fd1426, %fd76;
	add.f64 	%fd1428, %fd1428, 0d3FF0000000000000;

BB0_70:
	add.s32 	%r259, %r259, 1;
	setp.lt.s32	%p56, %r259, 6;
	setp.lt.s32	%p57, %r258, 2;
	and.pred  	%p58, %p56, %p57;
	add.s32 	%r258, %r258, 1;
	@%p58 bra 	BB0_67;

BB0_71:
	setp.lt.f64	%p59, %fd1428, 0d3FF0000000000000;
	mov.f64 	%fd1441, 0d7FF8000000000207;
	@%p59 bra 	BB0_80;

	div.rn.f64 	%fd87, %fd1427, %fd1428;
	div.rn.f64 	%fd88, %fd1426, %fd1428;
	mov.f64 	%fd1437, 0d0000000000000000;
	mov.f64 	%fd1436, %fd1437;
	mov.f64 	%fd1435, %fd1437;
	@%p1 bra 	BB0_78;

	mov.u32 	%r240, %tid.x;
	add.s32 	%r261, %r1, %r240;
	mov.f64 	%fd1437, 0d0000000000000000;
	mov.u32 	%r260, 1;
	mov.f64 	%fd1436, %fd1437;
	mov.f64 	%fd1435, %fd1437;

BB0_74:
	mul.wide.s32 	%rd32, %r261, 8;
	add.s64 	%rd33, %rd11, %rd32;
	add.s64 	%rd34, %rd12, %rd32;
	ld.global.f64 	%fd92, [%rd34];
	ld.global.f64 	%fd93, [%rd33];
	abs.f64 	%fd647, %fd93;
	setp.gtu.f64	%p61, %fd647, 0d7FF0000000000000;
	@%p61 bra 	BB0_77;

	abs.f64 	%fd648, %fd92;
	setp.gtu.f64	%p62, %fd648, 0d7FF0000000000000;
	@%p62 bra 	BB0_77;

	sub.f64 	%fd649, %fd93, %fd87;
	sub.f64 	%fd650, %fd92, %fd88;
	fma.rn.f64 	%fd1435, %fd649, %fd650, %fd1435;
	fma.rn.f64 	%fd1437, %fd649, %fd649, %fd1437;
	fma.rn.f64 	%fd1436, %fd650, %fd650, %fd1436;

BB0_77:
	add.s32 	%r261, %r261, 1;
	setp.lt.s32	%p63, %r261, 6;
	setp.lt.s32	%p64, %r260, 2;
	and.pred  	%p65, %p63, %p64;
	add.s32 	%r260, %r260, 1;
	@%p65 bra 	BB0_74;

BB0_78:
	setp.eq.f64	%p66, %fd1436, 0d0000000000000000;
	setp.eq.f64	%p67, %fd1437, 0d0000000000000000;
	or.pred  	%p68, %p67, %p66;
	mov.f64 	%fd1441, 0d7FF8000000000214;
	@%p68 bra 	BB0_80;

	mul.f64 	%fd652, %fd1436, %fd1437;
	sqrt.rn.f64 	%fd653, %fd652;
	div.rn.f64 	%fd1441, %fd1435, %fd653;

BB0_80:
	add.f64 	%fd105, %fd1441, 0d0000000000000000;
	mov.f64 	%fd1446, 0d0000000000000000;
	mov.u32 	%r264, 0;
	mov.f64 	%fd1447, %fd1446;
	@%p1 bra 	BB0_86;

	mov.u32 	%r242, %tid.x;
	add.s32 	%r263, %r1, %r242;
	mov.f64 	%fd1446, 0d0000000000000000;
	mov.u32 	%r264, 0;
	mov.u32 	%r262, 1;
	mov.f64 	%fd1447, %fd1446;

BB0_82:
	cvt.s64.s32	%rd1, %r263;
	mul.wide.s32 	%rd35, %r263, 8;
	add.s64 	%rd36, %rd9, %rd35;
	ld.global.f64 	%fd1445, [%rd36];
	abs.f64 	%fd658, %fd1445;
	setp.gtu.f64	%p70, %fd658, 0d7FF0000000000000;
	@%p70 bra 	BB0_84;

	shl.b64 	%rd37, %rd1, 3;
	add.s64 	%rd38, %rd10, %rd37;
	ld.global.f64 	%fd1444, [%rd38];
	abs.f64 	%fd659, %fd1444;
	setp.le.f64	%p71, %fd659, 0d7FF0000000000000;
	@%p71 bra 	BB0_85;

BB0_84:
	add.s32 	%r264, %r264, -1;
	mov.f64 	%fd1444, 0d0000000000000000;
	mov.f64 	%fd1445, %fd1444;

BB0_85:
	add.f64 	%fd1446, %fd1446, %fd1445;
	add.f64 	%fd1447, %fd1447, %fd1444;
	add.s32 	%r264, %r264, 1;
	cvt.u32.u64	%r167, %rd1;
	add.s32 	%r168, %r167, 1;
	setp.lt.s32	%p72, %r168, 6;
	setp.lt.s32	%p73, %r262, 2;
	and.pred  	%p74, %p73, %p72;
	add.s32 	%r263, %r263, 1;
	add.s32 	%r262, %r262, 1;
	@%p74 bra 	BB0_82;

BB0_86:
	setp.lt.s32	%p75, %r264, 1;
	mov.f64 	%fd1452, 0d7FF8000000000207;
	@%p75 bra 	BB0_94;

	cvt.rn.f64.s32	%fd116, %r264;
	mov.f64 	%fd1451, 0d0000000000000000;
	@%p1 bra 	BB0_93;

	mov.u32 	%r241, %tid.x;
	div.rn.f64 	%fd117, %fd1446, %fd116;
	div.rn.f64 	%fd118, %fd1447, %fd116;
	add.s32 	%r268, %r1, %r241;
	mov.f64 	%fd1451, 0d0000000000000000;
	mov.u32 	%r267, 1;

BB0_89:
	cvt.s64.s32	%rd2, %r268;
	mul.wide.s32 	%rd39, %r268, 8;
	add.s64 	%rd40, %rd9, %rd39;
	ld.global.f64 	%fd120, [%rd40];
	abs.f64 	%fd665, %fd120;
	setp.gtu.f64	%p77, %fd665, 0d7FF0000000000000;
	mov.f64 	%fd1449, %fd118;
	mov.f64 	%fd1450, %fd117;
	@%p77 bra 	BB0_92;

	shl.b64 	%rd41, %rd2, 3;
	add.s64 	%rd42, %rd10, %rd41;
	ld.global.f64 	%fd121, [%rd42];
	abs.f64 	%fd666, %fd121;
	setp.gtu.f64	%p78, %fd666, 0d7FF0000000000000;
	mov.f64 	%fd1449, %fd118;
	mov.f64 	%fd1450, %fd117;
	@%p78 bra 	BB0_92;

	mov.f64 	%fd1449, %fd121;
	mov.f64 	%fd1450, %fd120;

BB0_92:
	sub.f64 	%fd667, %fd1449, %fd118;
	sub.f64 	%fd668, %fd1450, %fd117;
	fma.rn.f64 	%fd1451, %fd667, %fd668, %fd1451;
	cvt.u32.u64	%r171, %rd2;
	add.s32 	%r172, %r171, 1;
	setp.lt.s32	%p79, %r172, 6;
	setp.lt.s32	%p80, %r267, 2;
	and.pred  	%p81, %p80, %p79;
	add.s32 	%r268, %r268, 1;
	add.s32 	%r267, %r267, 1;
	@%p81 bra 	BB0_89;

BB0_93:
	div.rn.f64 	%fd1452, %fd1451, %fd116;

BB0_94:
	mov.f64 	%fd130, 0d8000000000000000;
	setp.gt.s32	%p82, %r3, 4;
	@%p82 bra 	BB0_97;

	ld.param.u64 	%rd72, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3];
	mul.wide.s32 	%rd43, %r3, 8;
	add.s64 	%rd44, %rd72, %rd43;
	ld.global.f64 	%fd128, [%rd44];
	abs.f64 	%fd671, %fd128;
	setp.gtu.f64	%p83, %fd671, 0d7FF0000000000000;
	@%p83 bra 	BB0_97;

	mul.f64 	%fd130, %fd128, 0dBFE6A09E667F3BCC;

BB0_97:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r173}, %fd130; 
	}
	// inline asm
	setp.lt.s32	%p84, %r173, 1072168960;
	@%p84 bra 	BB0_104;
	bra.uni 	BB0_98;

BB0_104:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r74}, %fd130;
	}
	and.b32  	%r75, %r74, 2147483647;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r76, %temp}, %fd130;
	}
	setp.lt.u32	%p90, %r75, 1072693248;
	@%p90 bra 	BB0_110;
	bra.uni 	BB0_105;

BB0_110:
	mul.f64 	%fd928, %fd130, %fd130;
	mov.f64 	%fd929, 0d3E4D5F4BB7A316F6;
	mov.f64 	%fd930, 0dBE0A83AA3B08FBC2;
	fma.rn.f64 	%fd931, %fd930, %fd928, %fd929;
	mov.f64 	%fd932, 0dBE85BDCE301B3CDF;
	fma.rn.f64 	%fd933, %fd931, %fd928, %fd932;
	mov.f64 	%fd934, 0d3EBB978FADB81BC9;
	fma.rn.f64 	%fd935, %fd933, %fd928, %fd934;
	mov.f64 	%fd936, 0dBEEF4C99D6AE5FB8;
	fma.rn.f64 	%fd937, %fd935, %fd928, %fd936;
	mov.f64 	%fd938, 0d3F1F9A2AF549012E;
	fma.rn.f64 	%fd939, %fd937, %fd928, %fd938;
	mov.f64 	%fd940, 0dBF4C02DAFC636A47;
	fma.rn.f64 	%fd941, %fd939, %fd928, %fd940;
	mov.f64 	%fd942, 0d3F7565BCCF619AC0;
	fma.rn.f64 	%fd943, %fd941, %fd928, %fd942;
	mov.f64 	%fd944, 0dBF9B82CE311E321A;
	fma.rn.f64 	%fd945, %fd943, %fd928, %fd944;
	mov.f64 	%fd946, 0d3FBCE2F21A04075C;
	fma.rn.f64 	%fd947, %fd945, %fd928, %fd946;
	mov.f64 	%fd948, 0dBFD812746B0379B4;
	fma.rn.f64 	%fd949, %fd947, %fd928, %fd948;
	mov.f64 	%fd950, 0d3FF20DD750429B6D;
	fma.rn.f64 	%fd951, %fd949, %fd928, %fd950;
	mul.f64 	%fd1455, %fd130, %fd951;
	bra.uni 	BB0_111;

BB0_98:
	setp.gt.f64	%p85, %fd130, 0d403B4CCCCCCCCCCD;
	mov.f64 	%fd1456, 0d0000000000000000;
	@%p85 bra 	BB0_112;

	mul.rn.f64 	%fd678, %fd130, %fd130;
	neg.f64 	%fd677, %fd678;
	// inline asm
	fma.rn.f64 	%fd674, %fd130, %fd130, %fd677;
	// inline asm
	mov.f64 	%fd679, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd1454, %fd677, %fd679;
	abs.f64 	%fd134, %fd1454;
	setp.ge.f64	%p86, %fd134, 0d4330000000000000;
	@%p86 bra 	BB0_101;

	add.f64 	%fd680, %fd134, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd681, %fd680;
	setp.lt.f64	%p87, %fd134, 0d3FE0000000000000;
	selp.f64	%fd682, 0d0000000000000000, %fd681, %p87;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r174, %temp}, %fd682;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r175}, %fd682;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd1454;
	}
	and.b32  	%r177, %r176, -2147483648;
	or.b32  	%r178, %r175, %r177;
	mov.b64 	%fd1454, {%r174, %r178};

BB0_101:
	mov.f64 	%fd685, 0dBFE62E42FEFA39EF;
	// inline asm
	fma.rn.f64 	%fd683, %fd1454, %fd685, %fd677;
	// inline asm
	mov.f64 	%fd689, 0dBC7ABC9E3B39803F;
	// inline asm
	fma.rn.f64 	%fd687, %fd1454, %fd689, %fd683;
	// inline asm
	cvt.rzi.s32.f64	%r181, %fd1454;
	setp.lt.s32	%p88, %r181, -1020;
	add.s32 	%r182, %r181, 55;
	selp.f64	%fd744, 0d3C90000000000000, 0d4000000000000000, %p88;
	selp.b32	%r183, %r182, %r181, %p88;
	mov.f64 	%fd692, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd694, 0d3E5AFD81DA6C3BAF;
	// inline asm
	fma.rn.f64 	%fd691, %fd692, %fd687, %fd694;
	// inline asm
	mov.f64 	%fd698, 0d3E927E55F60F80E6;
	// inline asm
	fma.rn.f64 	%fd695, %fd691, %fd687, %fd698;
	// inline asm
	mov.f64 	%fd702, 0d3EC71DDA8F02D666;
	// inline asm
	fma.rn.f64 	%fd699, %fd695, %fd687, %fd702;
	// inline asm
	mov.f64 	%fd706, 0d3EFA01A013B894E0;
	// inline asm
	fma.rn.f64 	%fd703, %fd699, %fd687, %fd706;
	// inline asm
	mov.f64 	%fd710, 0d3F2A01A01D3AF788;
	// inline asm
	fma.rn.f64 	%fd707, %fd703, %fd687, %fd710;
	// inline asm
	mov.f64 	%fd714, 0d3F56C16C16C3A1EC;
	// inline asm
	fma.rn.f64 	%fd711, %fd707, %fd687, %fd714;
	// inline asm
	mov.f64 	%fd718, 0d3F81111111109161;
	// inline asm
	fma.rn.f64 	%fd715, %fd711, %fd687, %fd718;
	// inline asm
	mov.f64 	%fd722, 0d3FA55555555554C1;
	// inline asm
	fma.rn.f64 	%fd719, %fd715, %fd687, %fd722;
	// inline asm
	mov.f64 	%fd726, 0d3FC555555555556F;
	// inline asm
	fma.rn.f64 	%fd723, %fd719, %fd687, %fd726;
	// inline asm
	mov.f64 	%fd730, 0d3FE0000000000000;
	// inline asm
	fma.rn.f64 	%fd727, %fd723, %fd687, %fd730;
	// inline asm
	mul.rn.f64 	%fd732, %fd727, %fd687;
	// inline asm
	fma.rn.f64 	%fd731, %fd732, %fd687, %fd687;
	// inline asm
	shl.b32 	%r184, %r183, 20;
	add.s32 	%r180, %r184, 1071644672;
	mov.u32 	%r179, 0;
	// inline asm
	mov.b64 	%fd735, {%r179, %r180};
	// inline asm
	// inline asm
	fma.rn.f64 	%fd736, %fd731, %fd735, %fd735;
	// inline asm
	mul.rn.f64 	%fd743, %fd736, %fd744;
	neg.f64 	%fd742, %fd743;
	// inline asm
	fma.rn.f64 	%fd740, %fd674, %fd742, %fd743;
	// inline asm
	setp.lt.s32	%p89, %r173, 1075052544;
	@%p89 bra 	BB0_103;
	bra.uni 	BB0_102;

BB0_103:
	mov.f64 	%fd792, 0d3FE20DD7452FBC22;
	mov.f64 	%fd794, 0d401FD453E105E9A2;
	// inline asm
	fma.rn.f64 	%fd791, %fd792, %fd130, %fd794;
	// inline asm
	mov.f64 	%fd798, 0d404B26245B951FB4;
	// inline asm
	fma.rn.f64 	%fd795, %fd791, %fd130, %fd798;
	// inline asm
	mov.f64 	%fd802, 0d406C7835DC0F1F49;
	// inline asm
	fma.rn.f64 	%fd799, %fd795, %fd130, %fd802;
	// inline asm
	mov.f64 	%fd806, 0d4083AFA471E5C766;
	// inline asm
	fma.rn.f64 	%fd803, %fd799, %fd130, %fd806;
	// inline asm
	mov.f64 	%fd810, 0d4091FB514824F49F;
	// inline asm
	fma.rn.f64 	%fd807, %fd803, %fd130, %fd810;
	// inline asm
	mov.f64 	%fd814, 0d409450DDEE8272BB;
	// inline asm
	fma.rn.f64 	%fd811, %fd807, %fd130, %fd814;
	// inline asm
	mov.f64 	%fd818, 0d4086B952E4ECBC50;
	// inline asm
	fma.rn.f64 	%fd815, %fd811, %fd130, %fd818;
	// inline asm
	add.f64 	%fd820, %fd130, 0d402C35442E99E667;
	mov.f64 	%fd822, 0d40582F68071A079D;
	// inline asm
	fma.rn.f64 	%fd819, %fd820, %fd130, %fd822;
	// inline asm
	mov.f64 	%fd826, 0d4079ABD39A029DAA;
	// inline asm
	fma.rn.f64 	%fd823, %fd819, %fd130, %fd826;
	// inline asm
	mov.f64 	%fd830, 0d409230CA327093FD;
	// inline asm
	fma.rn.f64 	%fd827, %fd823, %fd130, %fd830;
	// inline asm
	mov.f64 	%fd834, 0d40A174FAB33B54A7;
	// inline asm
	fma.rn.f64 	%fd831, %fd827, %fd130, %fd834;
	// inline asm
	mov.f64 	%fd838, 0d40A601508230F980;
	// inline asm
	fma.rn.f64 	%fd835, %fd831, %fd130, %fd838;
	// inline asm
	mov.f64 	%fd842, 0d40A091785EC9331E;
	// inline asm
	fma.rn.f64 	%fd839, %fd835, %fd130, %fd842;
	// inline asm
	mov.f64 	%fd846, 0d4086B952E52F3622;
	// inline asm
	fma.rn.f64 	%fd843, %fd839, %fd130, %fd846;
	// inline asm
	div.rn.f64 	%fd847, %fd815, %fd843;
	mul.rn.f64 	%fd1456, %fd847, %fd740;
	bra.uni 	BB0_112;

BB0_105:
	setp.lt.u32	%p91, %r75, 2146435072;
	@%p91 bra 	BB0_109;
	bra.uni 	BB0_106;

BB0_109:
	mov.b64 	%fd849, {%r76, %r75};
	mov.f64 	%fd850, 0dBCF1384CE38C616A;
	mov.f64 	%fd851, 0d3C8B9C2B870030E8;
	fma.rn.f64 	%fd852, %fd851, %fd849, %fd850;
	mov.f64 	%fd853, 0d3D4458AE9746C2FD;
	fma.rn.f64 	%fd854, %fd852, %fd849, %fd853;
	mov.f64 	%fd855, 0dBD8E4A44D4F1AB56;
	fma.rn.f64 	%fd856, %fd854, %fd849, %fd855;
	mov.f64 	%fd857, 0d3DCFDF15265C58EE;
	fma.rn.f64 	%fd858, %fd856, %fd849, %fd857;
	mov.f64 	%fd859, 0dBE0933832F358D51;
	fma.rn.f64 	%fd860, %fd858, %fd849, %fd859;
	mov.f64 	%fd861, 0d3E3F136D3F719446;
	fma.rn.f64 	%fd862, %fd860, %fd849, %fd861;
	mov.f64 	%fd863, 0dBE6E94C2FE151B3B;
	fma.rn.f64 	%fd864, %fd862, %fd849, %fd863;
	mov.f64 	%fd865, 0d3E985A70310EE0A8;
	fma.rn.f64 	%fd866, %fd864, %fd849, %fd865;
	mov.f64 	%fd867, 0dBEBF944DA1520B74;
	fma.rn.f64 	%fd868, %fd866, %fd849, %fd867;
	mov.f64 	%fd869, 0d3EE09F503825C543;
	fma.rn.f64 	%fd870, %fd868, %fd849, %fd869;
	mov.f64 	%fd871, 0dBEFBEEFE9F949E59;
	fma.rn.f64 	%fd872, %fd870, %fd849, %fd871;
	mov.f64 	%fd873, 0d3F11D785C6E28857;
	fma.rn.f64 	%fd874, %fd872, %fd849, %fd873;
	mov.f64 	%fd875, 0dBF1D866B223048C7;
	fma.rn.f64 	%fd876, %fd874, %fd849, %fd875;
	mov.f64 	%fd877, 0d3EF258F0847E8908;
	fma.rn.f64 	%fd878, %fd876, %fd849, %fd877;
	mov.f64 	%fd879, 0d3F429CFC58DBB776;
	fma.rn.f64 	%fd880, %fd878, %fd849, %fd879;
	mov.f64 	%fd881, 0dBF5BE16D3F71F3C5;
	fma.rn.f64 	%fd882, %fd880, %fd849, %fd881;
	mov.f64 	%fd883, 0d3F2E8BDA60326B1A;
	fma.rn.f64 	%fd884, %fd882, %fd849, %fd883;
	mov.f64 	%fd885, 0d3F938FB20B0988A6;
	fma.rn.f64 	%fd886, %fd884, %fd849, %fd885;
	mov.f64 	%fd887, 0dBFBA4E3A80F64E33;
	fma.rn.f64 	%fd888, %fd886, %fd849, %fd887;
	mov.f64 	%fd889, 0dBFE45F3E88093928;
	fma.rn.f64 	%fd890, %fd888, %fd849, %fd889;
	mov.f64 	%fd891, 0dBFF20DD599CAEEA0;
	fma.rn.f64 	%fd892, %fd890, %fd849, %fd891;
	mov.f64 	%fd893, 0dBE883BE1E31CE133;
	fma.rn.f64 	%fd894, %fd892, %fd849, %fd893;
	mov.f64 	%fd895, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd896, %fd894, %fd895;
	mov.f64 	%fd897, 0d4338000000000000;
	add.rn.f64 	%fd898, %fd896, %fd897;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r189, %temp}, %fd898;
	}
	mov.f64 	%fd899, 0dC338000000000000;
	add.rn.f64 	%fd900, %fd898, %fd899;
	mov.f64 	%fd901, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd902, %fd900, %fd901, %fd894;
	mov.f64 	%fd903, 0d3E928AF3FCA213EA;
	mov.f64 	%fd904, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd905, %fd904, %fd902, %fd903;
	mov.f64 	%fd906, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd907, %fd905, %fd902, %fd906;
	mov.f64 	%fd908, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd909, %fd907, %fd902, %fd908;
	mov.f64 	%fd910, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd911, %fd909, %fd902, %fd910;
	mov.f64 	%fd912, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd913, %fd911, %fd902, %fd912;
	mov.f64 	%fd914, 0d3F81111111122322;
	fma.rn.f64 	%fd915, %fd913, %fd902, %fd914;
	mov.f64 	%fd916, 0d3FA55555555502A1;
	fma.rn.f64 	%fd917, %fd915, %fd902, %fd916;
	mov.f64 	%fd918, 0d3FC5555555555511;
	fma.rn.f64 	%fd919, %fd917, %fd902, %fd918;
	mov.f64 	%fd920, 0d3FE000000000000B;
	fma.rn.f64 	%fd921, %fd919, %fd902, %fd920;
	mov.f64 	%fd922, 0d3FF0000000000000;
	fma.rn.f64 	%fd923, %fd921, %fd902, %fd922;
	fma.rn.f64 	%fd924, %fd923, %fd902, %fd922;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r190}, %fd924;
	}
	shl.b32 	%r191, %r189, 20;
	add.s32 	%r192, %r190, %r191;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r193, %temp}, %fd924;
	}
	mov.b64 	%fd925, {%r193, %r192};
	sub.f64 	%fd926, %fd922, %fd925;
	setp.gt.u32	%p95, %r75, 1075294207;
	selp.f64	%fd927, 0d3FF0000000000000, %fd926, %p95;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r194, %temp}, %fd927;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r195}, %fd927;
	}
	and.b32  	%r196, %r74, -2147483648;
	or.b32  	%r197, %r195, %r196;
	mov.b64 	%fd1455, {%r194, %r197};
	bra.uni 	BB0_111;

BB0_102:
	rcp.rn.f64 	%fd789, %fd130;
	mul.rn.f64 	%fd787, %fd789, %fd789;
	mov.f64 	%fd746, 0dC1186DF84479631D;
	mov.f64 	%fd748, 0d41019A6E9A7FFBB8;
	// inline asm
	fma.rn.f64 	%fd745, %fd746, %fd787, %fd748;
	// inline asm
	mov.f64 	%fd752, 0dC0DB040BE3D5CA18;
	// inline asm
	fma.rn.f64 	%fd749, %fd745, %fd787, %fd752;
	// inline asm
	mov.f64 	%fd756, 0d40B012760EE009A0;
	// inline asm
	fma.rn.f64 	%fd753, %fd749, %fd787, %fd756;
	// inline asm
	mov.f64 	%fd760, 0dC082587AE4008D0E;
	// inline asm
	fma.rn.f64 	%fd757, %fd753, %fd787, %fd760;
	// inline asm
	mov.f64 	%fd764, 0d4056DF5D938ACAFE;
	// inline asm
	fma.rn.f64 	%fd761, %fd757, %fd787, %fd764;
	// inline asm
	mov.f64 	%fd768, 0dC030A8D46D765681;
	// inline asm
	fma.rn.f64 	%fd765, %fd761, %fd787, %fd768;
	// inline asm
	mov.f64 	%fd772, 0d400D9EAE0C665C75;
	// inline asm
	fma.rn.f64 	%fd769, %fd765, %fd787, %fd772;
	// inline asm
	mov.f64 	%fd776, 0dBFF0ECF9C8880942;
	// inline asm
	fma.rn.f64 	%fd773, %fd769, %fd787, %fd776;
	// inline asm
	mov.f64 	%fd780, 0d3FDB14C2F82A33F7;
	// inline asm
	fma.rn.f64 	%fd777, %fd773, %fd787, %fd780;
	// inline asm
	mov.f64 	%fd784, 0dBFD20DD75042844F;
	// inline asm
	fma.rn.f64 	%fd781, %fd777, %fd787, %fd784;
	// inline asm
	mov.f64 	%fd788, 0d3FE20DD750429B6B;
	// inline asm
	fma.rn.f64 	%fd785, %fd781, %fd787, %fd788;
	// inline asm
	mul.rn.f64 	%fd790, %fd785, %fd789;
	mul.rn.f64 	%fd1456, %fd790, %fd740;
	bra.uni 	BB0_112;

BB0_106:
	setp.eq.s32	%p92, %r75, 2146435072;
	setp.eq.s32	%p93, %r76, 0;
	and.pred  	%p94, %p92, %p93;
	@%p94 bra 	BB0_108;
	bra.uni 	BB0_107;

BB0_108:
	mov.f64 	%fd848, 0d3FF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r185, %temp}, %fd848;
	}
	and.b32  	%r186, %r74, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r187}, %fd848;
	}
	or.b32  	%r188, %r187, %r186;
	mov.b64 	%fd1455, {%r185, %r188};
	bra.uni 	BB0_111;

BB0_107:
	add.f64 	%fd1455, %fd130, %fd130;

BB0_111:
	mov.f64 	%fd952, 0d3FF0000000000000;
	sub.f64 	%fd1456, %fd952, %fd1455;

BB0_112:
	ld.param.u64 	%rd69, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2];
	fma.rn.f64 	%fd147, %fd1456, 0d3FE0000000000000, 0d0000000000000000;
	mul.wide.s32 	%rd45, %r3, 8;
	add.s64 	%rd3, %rd69, %rd45;
	mov.f64 	%fd1457, 0d0000000000000000;
	mov.f64 	%fd1458, %fd1457;
	@%p82 bra 	BB0_114;

	ld.global.f64 	%fd955, [%rd3];
	abs.f64 	%fd956, %fd955;
	setp.gtu.f64	%p97, %fd956, 0d7FF0000000000000;
	add.f64 	%fd957, %fd955, 0d0000000000000000;
	selp.f64	%fd1457, 0d0000000000000000, %fd957, %p97;
	selp.f64	%fd1458, 0d0000000000000000, 0d3FF0000000000000, %p97;

BB0_114:
	ld.param.u64 	%rd70, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1];
	add.s64 	%rd4, %rd70, %rd45;
	@%p82 bra 	BB0_116;

	ld.global.f64 	%fd958, [%rd4];
	abs.f64 	%fd959, %fd958;
	setp.gtu.f64	%p99, %fd959, 0d7FF0000000000000;
	add.f64 	%fd960, %fd1457, %fd958;
	selp.f64	%fd1457, %fd1457, %fd960, %p99;
	add.f64 	%fd961, %fd1458, 0d3FF0000000000000;
	selp.f64	%fd1458, %fd1458, %fd961, %p99;

BB0_116:
	setp.eq.f64	%p100, %fd1458, 0d3FF0000000000000;
	mov.f64 	%fd1465, 0d3FF0000000000000;
	@%p100 bra 	BB0_142;

	abs.f64 	%fd156, %fd1458;
	setp.gtu.f64	%p101, %fd156, 0d7FF0000000000000;
	@%p101 bra 	BB0_141;
	bra.uni 	BB0_118;

BB0_141:
	add.f64 	%fd1465, %fd1458, 0dBFF0000000000000;
	bra.uni 	BB0_142;

BB0_118:
	setp.eq.f64	%p102, %fd1458, 0d7FF0000000000000;
	@%p102 bra 	BB0_140;
	bra.uni 	BB0_119;

BB0_140:
	mov.f64 	%fd1153, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r218}, %fd1153;
	}
	setp.gt.s32	%p119, %r218, -1;
	selp.f64	%fd1465, 0d7FF0000000000000, 0d0000000000000000, %p119;
	bra.uni 	BB0_142;

BB0_119:
	mov.f64 	%fd963, 0dBFF0000000000000;
	mov.f64 	%fd964, 0d3FE0000000000000;
	mul.rn.f64 	%fd965, %fd964, %fd963;
	cvt.rzi.f64.f64	%fd966, %fd965;
	mov.f64 	%fd967, 0d4000000000000000;
	mul.rn.f64 	%fd968, %fd967, %fd966;
	sub.f64 	%fd969, %fd963, %fd968;
	abs.f64 	%fd157, %fd969;
	setp.eq.f64	%p103, %fd1458, 0d0000000000000000;
	@%p103 bra 	BB0_139;
	bra.uni 	BB0_120;

BB0_139:
	setp.eq.f64	%p118, %fd157, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1150, %fd1458;
	mov.f64 	%fd1151, 0d0000000000000000;
	rcp.rn.f64 	%fd1152, %fd1151;
	selp.f64	%fd1465, %fd1150, %fd1152, %p118;
	bra.uni 	BB0_142;

BB0_120:
	setp.eq.f64	%p104, %fd1458, 0dFFF0000000000000;
	@%p104 bra 	BB0_137;
	bra.uni 	BB0_121;

BB0_137:
	div.rn.f64 	%fd1465, %fd963, %fd1458;
	setp.neu.f64	%p117, %fd157, 0d3FF0000000000000;
	@%p117 bra 	BB0_142;

	mov.b64 	 %rd49, %fd1465;
	xor.b64  	%rd50, %rd49, -9223372036854775808;
	mov.b64 	 %fd1465, %rd50;
	bra.uni 	BB0_142;

BB0_121:
	setp.geu.f64	%p105, %fd1458, 0d0000000000000000;
	@%p105 bra 	BB0_123;

	cvt.rzi.f64.f64	%fd972, %fd963;
	setp.neu.f64	%p106, %fd972, 0dBFF0000000000000;
	mov.f64 	%fd1465, 0dFFF8000000000000;
	@%p106 bra 	BB0_142;

BB0_123:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r270}, %fd156; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r269, hi}, %fd156; 
	}
	// inline asm
	bfe.u32 	%r271, %r270, 20, 11;
	setp.ne.s32	%p107, %r271, 0;
	@%p107 bra 	BB0_125;

	mov.f64 	%fd977, 0d4350000000000000;
	mul.rn.f64 	%fd976, %fd156, %fd977;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r270}, %fd976; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r269, hi}, %fd976; 
	}
	// inline asm
	bfe.u32 	%r202, %r270, 20, 11;
	add.s32 	%r271, %r202, -54;

BB0_125:
	add.s32 	%r272, %r271, -1023;
	and.b32  	%r205, %r270, -2146435073;
	or.b32  	%r204, %r205, 1072693248;
	// inline asm
	mov.b64 	%fd1461, {%r269, %r204};
	// inline asm
	setp.lt.u32	%p108, %r204, 1073127583;
	@%p108 bra 	BB0_127;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r206, hi}, %fd1461; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r207}, %fd1461; 
	}
	// inline asm
	add.s32 	%r209, %r207, -1048576;
	// inline asm
	mov.b64 	%fd1461, {%r206, %r209};
	// inline asm
	add.s32 	%r272, %r271, -1022;

BB0_127:
	add.f64 	%fd1066, %fd1461, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1067, %fd1066;
	add.f64 	%fd1008, %fd1461, 0dBFF0000000000000;
	mul.rn.f64 	%fd1068, %fd1008, %fd1067;
	add.f64 	%fd1056, %fd1068, %fd1068;
	mul.rn.f64 	%fd1004, %fd1056, %fd1056;
	mov.f64 	%fd983, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd985, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd982, %fd983, %fd1004, %fd985;
	// inline asm
	mov.f64 	%fd989, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd986, %fd982, %fd1004, %fd989;
	// inline asm
	mov.f64 	%fd993, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd990, %fd986, %fd1004, %fd993;
	// inline asm
	mov.f64 	%fd997, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd994, %fd990, %fd1004, %fd997;
	// inline asm
	mov.f64 	%fd1001, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd998, %fd994, %fd1004, %fd1001;
	// inline asm
	mov.f64 	%fd1005, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd1002, %fd998, %fd1004, %fd1005;
	// inline asm
	mul.rn.f64 	%fd1069, %fd1002, %fd1004;
	sub.f64 	%fd1070, %fd1008, %fd1056;
	mul.rn.f64 	%fd1009, %fd967, %fd1070;
	neg.f64 	%fd1007, %fd1056;
	// inline asm
	fma.rn.f64 	%fd1006, %fd1007, %fd1008, %fd1009;
	// inline asm
	mul.rn.f64 	%fd1052, %fd1067, %fd1006;
	add.f64 	%fd1072, %fd1069, 0d3FB5555555555555;
	mov.f64 	%fd1073, 0d3FB5555555555555;
	sub.f64 	%fd1074, %fd1073, %fd1072;
	add.f64 	%fd1075, %fd1069, %fd1074;
	add.f64 	%fd1076, %fd1075, 0d0000000000000000;
	add.f64 	%fd1077, %fd1076, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1019, %fd1072, %fd1077;
	sub.f64 	%fd1078, %fd1072, %fd1019;
	add.f64 	%fd1023, %fd1077, %fd1078;
	mul.rn.f64 	%fd1079, %fd1019, %fd1056;
	neg.f64 	%fd1013, %fd1079;
	// inline asm
	fma.rn.f64 	%fd1010, %fd1019, %fd1056, %fd1013;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1014, %fd1023, %fd1052, %fd1010;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1018, %fd1019, %fd1052, %fd1014;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1022, %fd1023, %fd1056, %fd1018;
	// inline asm
	add.f64 	%fd1035, %fd1079, %fd1022;
	sub.f64 	%fd1080, %fd1079, %fd1035;
	add.f64 	%fd1039, %fd1022, %fd1080;
	mul.rn.f64 	%fd1081, %fd1035, %fd1056;
	neg.f64 	%fd1029, %fd1081;
	// inline asm
	fma.rn.f64 	%fd1026, %fd1035, %fd1056, %fd1029;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1030, %fd1039, %fd1052, %fd1026;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1034, %fd1035, %fd1052, %fd1030;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1038, %fd1039, %fd1056, %fd1034;
	// inline asm
	add.f64 	%fd1051, %fd1081, %fd1038;
	sub.f64 	%fd1082, %fd1081, %fd1051;
	add.f64 	%fd1055, %fd1038, %fd1082;
	mul.rn.f64 	%fd1083, %fd1051, %fd1056;
	neg.f64 	%fd1045, %fd1083;
	// inline asm
	fma.rn.f64 	%fd1042, %fd1051, %fd1056, %fd1045;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1046, %fd1055, %fd1052, %fd1042;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1050, %fd1051, %fd1052, %fd1046;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1054, %fd1055, %fd1056, %fd1050;
	// inline asm
	add.f64 	%fd1084, %fd1083, %fd1054;
	sub.f64 	%fd1085, %fd1083, %fd1084;
	add.f64 	%fd1086, %fd1054, %fd1085;
	add.f64 	%fd1087, %fd1056, %fd1084;
	sub.f64 	%fd1088, %fd1056, %fd1087;
	add.f64 	%fd1089, %fd1084, %fd1088;
	add.f64 	%fd1090, %fd1086, %fd1089;
	add.f64 	%fd1091, %fd1052, %fd1090;
	add.f64 	%fd1092, %fd1087, %fd1091;
	sub.f64 	%fd1093, %fd1087, %fd1092;
	add.f64 	%fd1094, %fd1091, %fd1093;
	cvt.rn.f64.s32	%fd1095, %r272;
	mov.f64 	%fd1096, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1097, %fd1095, %fd1096;
	mov.f64 	%fd1098, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1099, %fd1095, %fd1098;
	add.f64 	%fd1100, %fd1097, %fd1092;
	sub.f64 	%fd1101, %fd1097, %fd1100;
	add.f64 	%fd1102, %fd1092, %fd1101;
	add.f64 	%fd1103, %fd1094, %fd1102;
	add.f64 	%fd1104, %fd1099, %fd1103;
	add.f64 	%fd1059, %fd1100, %fd1104;
	sub.f64 	%fd1105, %fd1100, %fd1059;
	add.f64 	%fd1063, %fd1104, %fd1105;
	mul.rn.f64 	%fd1106, %fd1059, %fd963;
	neg.f64 	%fd1061, %fd1106;
	// inline asm
	fma.rn.f64 	%fd1058, %fd1059, %fd963, %fd1061;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1062, %fd1063, %fd963, %fd1058;
	// inline asm
	add.f64 	%fd161, %fd1106, %fd1062;
	sub.f64 	%fd1107, %fd1106, %fd161;
	add.f64 	%fd162, %fd1062, %fd1107;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r89}, %fd161;
	}
	mov.b32 	 %f5, %r89;
	abs.f32 	%f6, %f5;
	setp.lt.f32	%p109, %f6, 0f40874911;
	@%p109 bra 	BB0_129;
	bra.uni 	BB0_128;

BB0_129:
	mov.f64 	%fd1111, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd1112, %fd161, %fd1111;
	mov.f64 	%fd1113, 0d4338000000000000;
	add.rn.f64 	%fd1114, %fd1112, %fd1113;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r90, %temp}, %fd1114;
	}
	mov.f64 	%fd1115, 0dC338000000000000;
	add.rn.f64 	%fd1116, %fd1114, %fd1115;
	mov.f64 	%fd1117, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1118, %fd1116, %fd1117, %fd161;
	mov.f64 	%fd1119, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1120, %fd1116, %fd1119, %fd1118;
	mov.f64 	%fd1121, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1122, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1123, %fd1122, %fd1120, %fd1121;
	mov.f64 	%fd1124, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1125, %fd1123, %fd1120, %fd1124;
	mov.f64 	%fd1126, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1127, %fd1125, %fd1120, %fd1126;
	mov.f64 	%fd1128, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1129, %fd1127, %fd1120, %fd1128;
	mov.f64 	%fd1130, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1131, %fd1129, %fd1120, %fd1130;
	mov.f64 	%fd1132, 0d3F81111111122322;
	fma.rn.f64 	%fd1133, %fd1131, %fd1120, %fd1132;
	mov.f64 	%fd1134, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1135, %fd1133, %fd1120, %fd1134;
	mov.f64 	%fd1136, 0d3FC5555555555511;
	fma.rn.f64 	%fd1137, %fd1135, %fd1120, %fd1136;
	mov.f64 	%fd1138, 0d3FE000000000000B;
	fma.rn.f64 	%fd1139, %fd1137, %fd1120, %fd1138;
	mov.f64 	%fd1140, 0d3FF0000000000000;
	fma.rn.f64 	%fd1141, %fd1139, %fd1120, %fd1140;
	fma.rn.f64 	%fd1462, %fd1141, %fd1120, %fd1140;
	abs.s32 	%r210, %r90;
	setp.lt.s32	%p112, %r210, 1023;
	@%p112 bra 	BB0_131;
	bra.uni 	BB0_130;

BB0_131:
	shl.b32 	%r216, %r90, 20;
	add.s32 	%r273, %r216, 1072693248;
	bra.uni 	BB0_132;

BB0_128:
	setp.lt.s32	%p110, %r89, 0;
	selp.f64	%fd1108, 0d0000000000000000, 0d7FF0000000000000, %p110;
	abs.f64 	%fd1109, %fd161;
	setp.gtu.f64	%p111, %fd1109, 0d7FF0000000000000;
	add.f64 	%fd1110, %fd161, %fd161;
	selp.f64	%fd1465, %fd1110, %fd1108, %p111;
	bra.uni 	BB0_133;

BB0_130:
	add.s32 	%r211, %r90, 2046;
	shl.b32 	%r212, %r211, 19;
	and.b32  	%r213, %r212, -1048576;
	shl.b32 	%r214, %r211, 20;
	sub.s32 	%r273, %r214, %r213;
	mov.u32 	%r215, 0;
	mov.b64 	%fd1142, {%r215, %r213};
	mul.f64 	%fd1462, %fd1462, %fd1142;

BB0_132:
	mov.u32 	%r217, 0;
	mov.b64 	%fd1143, {%r217, %r273};
	mul.f64 	%fd1465, %fd1462, %fd1143;

BB0_133:
	abs.f64 	%fd1144, %fd1465;
	setp.eq.f64	%p113, %fd1144, 0d7FF0000000000000;
	@%p113 bra 	BB0_135;

	// inline asm
	fma.rn.f64 	%fd1465, %fd1465, %fd162, %fd1465;
	// inline asm

BB0_135:
	setp.neu.f64	%p114, %fd157, 0d3FF0000000000000;
	or.pred  	%p116, %p105, %p114;
	@%p116 bra 	BB0_142;

	mov.b64 	 %rd47, %fd1465;
	xor.b64  	%rd48, %rd47, -9223372036854775808;
	mov.b64 	 %fd1465, %rd48;

BB0_142:
	mul.f64 	%fd178, %fd1457, %fd1465;
	mov.f64 	%fd1466, 0d0000000000000000;
	@%p82 bra 	BB0_145;

	ld.global.f64 	%fd179, [%rd3];
	abs.f64 	%fd1156, %fd179;
	setp.gtu.f64	%p121, %fd1156, 0d7FF0000000000000;
	@%p121 bra 	BB0_145;

	sub.f64 	%fd1157, %fd179, %fd178;
	fma.rn.f64 	%fd1466, %fd1157, %fd1157, 0d0000000000000000;

BB0_145:
	@%p82 bra 	BB0_148;

	ld.global.f64 	%fd182, [%rd4];
	abs.f64 	%fd1158, %fd182;
	setp.gtu.f64	%p123, %fd1158, 0d7FF0000000000000;
	@%p123 bra 	BB0_148;

	sub.f64 	%fd1159, %fd182, %fd178;
	fma.rn.f64 	%fd1466, %fd1159, %fd1159, %fd1466;

BB0_148:
	mov.f64 	%fd1473, 0d7FF8000000000214;
	setp.le.f64	%p124, %fd1458, 0d3FF0000000000000;
	@%p124 bra 	BB0_176;

	add.f64 	%fd185, %fd1458, 0dBFF0000000000000;
	setp.eq.f64	%p125, %fd185, 0d3FF0000000000000;
	mov.f64 	%fd1472, 0d3FF0000000000000;
	@%p125 bra 	BB0_175;

	abs.f64 	%fd186, %fd185;
	setp.gtu.f64	%p126, %fd186, 0d7FF0000000000000;
	@%p126 bra 	BB0_174;
	bra.uni 	BB0_151;

BB0_174:
	add.f64 	%fd1472, %fd185, 0dBFF0000000000000;
	bra.uni 	BB0_175;

BB0_151:
	setp.eq.f64	%p127, %fd185, 0d7FF0000000000000;
	@%p127 bra 	BB0_173;
	bra.uni 	BB0_152;

BB0_173:
	mov.f64 	%fd1352, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r239}, %fd1352;
	}
	setp.gt.s32	%p144, %r239, -1;
	selp.f64	%fd1472, 0d7FF0000000000000, 0d0000000000000000, %p144;
	bra.uni 	BB0_175;

BB0_152:
	mov.f64 	%fd1162, 0dBFF0000000000000;
	mov.f64 	%fd1163, 0d3FE0000000000000;
	mul.rn.f64 	%fd1164, %fd1163, %fd1162;
	cvt.rzi.f64.f64	%fd1165, %fd1164;
	mov.f64 	%fd1166, 0d4000000000000000;
	mul.rn.f64 	%fd1167, %fd1166, %fd1165;
	sub.f64 	%fd1168, %fd1162, %fd1167;
	abs.f64 	%fd187, %fd1168;
	setp.eq.f64	%p128, %fd185, 0d0000000000000000;
	@%p128 bra 	BB0_172;
	bra.uni 	BB0_153;

BB0_172:
	setp.eq.f64	%p143, %fd187, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1349, %fd185;
	mov.f64 	%fd1350, 0d0000000000000000;
	rcp.rn.f64 	%fd1351, %fd1350;
	selp.f64	%fd1472, %fd1349, %fd1351, %p143;
	bra.uni 	BB0_175;

BB0_153:
	setp.eq.f64	%p129, %fd185, 0dFFF0000000000000;
	@%p129 bra 	BB0_170;
	bra.uni 	BB0_154;

BB0_170:
	div.rn.f64 	%fd1472, %fd1162, %fd185;
	setp.neu.f64	%p142, %fd187, 0d3FF0000000000000;
	@%p142 bra 	BB0_175;

	mov.b64 	 %rd53, %fd1472;
	xor.b64  	%rd54, %rd53, -9223372036854775808;
	mov.b64 	 %fd1472, %rd54;
	bra.uni 	BB0_175;

BB0_154:
	setp.geu.f64	%p130, %fd185, 0d0000000000000000;
	@%p130 bra 	BB0_156;

	cvt.rzi.f64.f64	%fd1171, %fd1162;
	setp.neu.f64	%p131, %fd1171, 0dBFF0000000000000;
	mov.f64 	%fd1472, 0dFFF8000000000000;
	@%p131 bra 	BB0_175;

BB0_156:
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r275}, %fd186; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r274, hi}, %fd186; 
	}
	// inline asm
	bfe.u32 	%r276, %r275, 20, 11;
	setp.ne.s32	%p132, %r276, 0;
	@%p132 bra 	BB0_158;

	mov.f64 	%fd1176, 0d4350000000000000;
	mul.rn.f64 	%fd1175, %fd186, %fd1176;
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r275}, %fd1175; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r274, hi}, %fd1175; 
	}
	// inline asm
	bfe.u32 	%r223, %r275, 20, 11;
	add.s32 	%r276, %r223, -54;

BB0_158:
	add.s32 	%r277, %r276, -1023;
	and.b32  	%r226, %r275, -2146435073;
	or.b32  	%r225, %r226, 1072693248;
	// inline asm
	mov.b64 	%fd1468, {%r274, %r225};
	// inline asm
	setp.lt.u32	%p133, %r225, 1073127583;
	@%p133 bra 	BB0_160;

	// inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r227, hi}, %fd1468; 
	}
	// inline asm
	// inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r228}, %fd1468; 
	}
	// inline asm
	add.s32 	%r230, %r228, -1048576;
	// inline asm
	mov.b64 	%fd1468, {%r227, %r230};
	// inline asm
	add.s32 	%r277, %r276, -1022;

BB0_160:
	add.f64 	%fd1265, %fd1468, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1266, %fd1265;
	add.f64 	%fd1207, %fd1468, 0dBFF0000000000000;
	mul.rn.f64 	%fd1267, %fd1207, %fd1266;
	add.f64 	%fd1255, %fd1267, %fd1267;
	mul.rn.f64 	%fd1203, %fd1255, %fd1255;
	mov.f64 	%fd1182, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1184, 0d3ED0F5D241AD3B5A;
	// inline asm
	fma.rn.f64 	%fd1181, %fd1182, %fd1203, %fd1184;
	// inline asm
	mov.f64 	%fd1188, 0d3EF3B20A75488A3F;
	// inline asm
	fma.rn.f64 	%fd1185, %fd1181, %fd1203, %fd1188;
	// inline asm
	mov.f64 	%fd1192, 0d3F1745CDE4FAECD5;
	// inline asm
	fma.rn.f64 	%fd1189, %fd1185, %fd1203, %fd1192;
	// inline asm
	mov.f64 	%fd1196, 0d3F3C71C7258A578B;
	// inline asm
	fma.rn.f64 	%fd1193, %fd1189, %fd1203, %fd1196;
	// inline asm
	mov.f64 	%fd1200, 0d3F6249249242B910;
	// inline asm
	fma.rn.f64 	%fd1197, %fd1193, %fd1203, %fd1200;
	// inline asm
	mov.f64 	%fd1204, 0d3F89999999999DFB;
	// inline asm
	fma.rn.f64 	%fd1201, %fd1197, %fd1203, %fd1204;
	// inline asm
	mul.rn.f64 	%fd1268, %fd1201, %fd1203;
	sub.f64 	%fd1269, %fd1207, %fd1255;
	mul.rn.f64 	%fd1208, %fd1166, %fd1269;
	neg.f64 	%fd1206, %fd1255;
	// inline asm
	fma.rn.f64 	%fd1205, %fd1206, %fd1207, %fd1208;
	// inline asm
	mul.rn.f64 	%fd1251, %fd1266, %fd1205;
	add.f64 	%fd1271, %fd1268, 0d3FB5555555555555;
	mov.f64 	%fd1272, 0d3FB5555555555555;
	sub.f64 	%fd1273, %fd1272, %fd1271;
	add.f64 	%fd1274, %fd1268, %fd1273;
	add.f64 	%fd1275, %fd1274, 0d0000000000000000;
	add.f64 	%fd1276, %fd1275, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1218, %fd1271, %fd1276;
	sub.f64 	%fd1277, %fd1271, %fd1218;
	add.f64 	%fd1222, %fd1276, %fd1277;
	mul.rn.f64 	%fd1278, %fd1218, %fd1255;
	neg.f64 	%fd1212, %fd1278;
	// inline asm
	fma.rn.f64 	%fd1209, %fd1218, %fd1255, %fd1212;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1213, %fd1222, %fd1251, %fd1209;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1217, %fd1218, %fd1251, %fd1213;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1221, %fd1222, %fd1255, %fd1217;
	// inline asm
	add.f64 	%fd1234, %fd1278, %fd1221;
	sub.f64 	%fd1279, %fd1278, %fd1234;
	add.f64 	%fd1238, %fd1221, %fd1279;
	mul.rn.f64 	%fd1280, %fd1234, %fd1255;
	neg.f64 	%fd1228, %fd1280;
	// inline asm
	fma.rn.f64 	%fd1225, %fd1234, %fd1255, %fd1228;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1229, %fd1238, %fd1251, %fd1225;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1233, %fd1234, %fd1251, %fd1229;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1237, %fd1238, %fd1255, %fd1233;
	// inline asm
	add.f64 	%fd1250, %fd1280, %fd1237;
	sub.f64 	%fd1281, %fd1280, %fd1250;
	add.f64 	%fd1254, %fd1237, %fd1281;
	mul.rn.f64 	%fd1282, %fd1250, %fd1255;
	neg.f64 	%fd1244, %fd1282;
	// inline asm
	fma.rn.f64 	%fd1241, %fd1250, %fd1255, %fd1244;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1245, %fd1254, %fd1251, %fd1241;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1249, %fd1250, %fd1251, %fd1245;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1253, %fd1254, %fd1255, %fd1249;
	// inline asm
	add.f64 	%fd1283, %fd1282, %fd1253;
	sub.f64 	%fd1284, %fd1282, %fd1283;
	add.f64 	%fd1285, %fd1253, %fd1284;
	add.f64 	%fd1286, %fd1255, %fd1283;
	sub.f64 	%fd1287, %fd1255, %fd1286;
	add.f64 	%fd1288, %fd1283, %fd1287;
	add.f64 	%fd1289, %fd1285, %fd1288;
	add.f64 	%fd1290, %fd1251, %fd1289;
	add.f64 	%fd1291, %fd1286, %fd1290;
	sub.f64 	%fd1292, %fd1286, %fd1291;
	add.f64 	%fd1293, %fd1290, %fd1292;
	cvt.rn.f64.s32	%fd1294, %r277;
	mov.f64 	%fd1295, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1296, %fd1294, %fd1295;
	mov.f64 	%fd1297, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1298, %fd1294, %fd1297;
	add.f64 	%fd1299, %fd1296, %fd1291;
	sub.f64 	%fd1300, %fd1296, %fd1299;
	add.f64 	%fd1301, %fd1291, %fd1300;
	add.f64 	%fd1302, %fd1293, %fd1301;
	add.f64 	%fd1303, %fd1298, %fd1302;
	add.f64 	%fd1258, %fd1299, %fd1303;
	sub.f64 	%fd1304, %fd1299, %fd1258;
	add.f64 	%fd1262, %fd1303, %fd1304;
	mul.rn.f64 	%fd1305, %fd1258, %fd1162;
	neg.f64 	%fd1260, %fd1305;
	// inline asm
	fma.rn.f64 	%fd1257, %fd1258, %fd1162, %fd1260;
	// inline asm
	// inline asm
	fma.rn.f64 	%fd1261, %fd1262, %fd1162, %fd1257;
	// inline asm
	add.f64 	%fd191, %fd1305, %fd1261;
	sub.f64 	%fd1306, %fd1305, %fd191;
	add.f64 	%fd192, %fd1261, %fd1306;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r106}, %fd191;
	}
	mov.b32 	 %f7, %r106;
	abs.f32 	%f8, %f7;
	setp.lt.f32	%p134, %f8, 0f40874911;
	@%p134 bra 	BB0_162;
	bra.uni 	BB0_161;

BB0_162:
	mov.f64 	%fd1310, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd1311, %fd191, %fd1310;
	mov.f64 	%fd1312, 0d4338000000000000;
	add.rn.f64 	%fd1313, %fd1311, %fd1312;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r107, %temp}, %fd1313;
	}
	mov.f64 	%fd1314, 0dC338000000000000;
	add.rn.f64 	%fd1315, %fd1313, %fd1314;
	mov.f64 	%fd1316, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1317, %fd1315, %fd1316, %fd191;
	mov.f64 	%fd1318, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1319, %fd1315, %fd1318, %fd1317;
	mov.f64 	%fd1320, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1321, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1322, %fd1321, %fd1319, %fd1320;
	mov.f64 	%fd1323, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1324, %fd1322, %fd1319, %fd1323;
	mov.f64 	%fd1325, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1326, %fd1324, %fd1319, %fd1325;
	mov.f64 	%fd1327, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1328, %fd1326, %fd1319, %fd1327;
	mov.f64 	%fd1329, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1330, %fd1328, %fd1319, %fd1329;
	mov.f64 	%fd1331, 0d3F81111111122322;
	fma.rn.f64 	%fd1332, %fd1330, %fd1319, %fd1331;
	mov.f64 	%fd1333, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1334, %fd1332, %fd1319, %fd1333;
	mov.f64 	%fd1335, 0d3FC5555555555511;
	fma.rn.f64 	%fd1336, %fd1334, %fd1319, %fd1335;
	mov.f64 	%fd1337, 0d3FE000000000000B;
	fma.rn.f64 	%fd1338, %fd1336, %fd1319, %fd1337;
	mov.f64 	%fd1339, 0d3FF0000000000000;
	fma.rn.f64 	%fd1340, %fd1338, %fd1319, %fd1339;
	fma.rn.f64 	%fd1469, %fd1340, %fd1319, %fd1339;
	abs.s32 	%r231, %r107;
	setp.lt.s32	%p137, %r231, 1023;
	@%p137 bra 	BB0_164;
	bra.uni 	BB0_163;

BB0_164:
	shl.b32 	%r237, %r107, 20;
	add.s32 	%r278, %r237, 1072693248;
	bra.uni 	BB0_165;

BB0_161:
	setp.lt.s32	%p135, %r106, 0;
	selp.f64	%fd1307, 0d0000000000000000, 0d7FF0000000000000, %p135;
	abs.f64 	%fd1308, %fd191;
	setp.gtu.f64	%p136, %fd1308, 0d7FF0000000000000;
	add.f64 	%fd1309, %fd191, %fd191;
	selp.f64	%fd1472, %fd1309, %fd1307, %p136;
	bra.uni 	BB0_166;

BB0_163:
	add.s32 	%r232, %r107, 2046;
	shl.b32 	%r233, %r232, 19;
	and.b32  	%r234, %r233, -1048576;
	shl.b32 	%r235, %r232, 20;
	sub.s32 	%r278, %r235, %r234;
	mov.u32 	%r236, 0;
	mov.b64 	%fd1341, {%r236, %r234};
	mul.f64 	%fd1469, %fd1469, %fd1341;

BB0_165:
	mov.u32 	%r238, 0;
	mov.b64 	%fd1342, {%r238, %r278};
	mul.f64 	%fd1472, %fd1469, %fd1342;

BB0_166:
	abs.f64 	%fd1343, %fd1472;
	setp.eq.f64	%p138, %fd1343, 0d7FF0000000000000;
	@%p138 bra 	BB0_168;

	// inline asm
	fma.rn.f64 	%fd1472, %fd1472, %fd192, %fd1472;
	// inline asm

BB0_168:
	setp.neu.f64	%p139, %fd187, 0d3FF0000000000000;
	or.pred  	%p141, %p130, %p139;
	@%p141 bra 	BB0_175;

	mov.b64 	 %rd51, %fd1472;
	xor.b64  	%rd52, %rd51, -9223372036854775808;
	mov.b64 	 %fd1472, %rd52;

BB0_175:
	mul.f64 	%fd1473, %fd1466, %fd1472;

BB0_176:
	setp.gt.f64	%p145, %fd147, 0d0000000000000000;
	setp.lt.f64	%p146, %fd1473, 0d0000000000000000;
	and.pred  	%p147, %p146, %p145;
	@%p147 bra 	BB0_178;

	setp.geu.f64	%p148, %fd147, 0d0000000000000000;
	setp.leu.f64	%p149, %fd1473, 0d0000000000000000;
	or.pred  	%p150, %p148, %p149;
	@%p150 bra 	BB0_187;

BB0_178:
	neg.f64 	%fd210, %fd147;
	setp.eq.f64	%p151, %fd1473, %fd210;
	mov.f64 	%fd1474, 0d0000000000000000;
	@%p151 bra 	BB0_188;

	setp.eq.f64	%p152, %fd1473, 0d0000000000000000;
	setp.eq.f64	%p153, %fd147, 0d8000000000000000;
	or.pred  	%p154, %p152, %p153;
	@%p154 bra 	BB0_187;

	add.f64 	%fd1354, %fd147, %fd1473;
	abs.f64 	%fd211, %fd1354;
	abs.f64 	%fd1355, %fd211;
	setp.geu.f64	%p155, %fd1355, 0d7FF0000000000000;
	@%p155 bra 	BB0_187;

	abs.f64 	%fd212, %fd1473;
	mul.f64 	%fd1356, %fd212, 0d3D30000000000000;
	setp.gt.f64	%p156, %fd211, %fd1356;
	@%p156 bra 	BB0_187;

	abs.f64 	%fd213, %fd210;
	mul.f64 	%fd1357, %fd213, 0d3D30000000000000;
	setp.gt.f64	%p157, %fd211, %fd1357;
	@%p157 bra 	BB0_187;

	mov.b64 	 %rd55, %fd211;
	setp.gt.s64	%p158, %rd55, 9007199254740991;
	setp.gtu.f64	%p159, %fd211, 0d001FFFFFFFFFFFFF;
	or.pred  	%p160, %p158, %p159;
	@%p160 bra 	BB0_185;

	mov.b64 	 %rd56, %fd212;
	setp.lt.s64	%p161, %rd56, 9007199254740992;
	setp.le.f64	%p162, %fd212, 0d001FFFFFFFFFFFFF;
	and.pred  	%p163, %p161, %p162;
	@%p163 bra 	BB0_186;
	bra.uni 	BB0_185;

BB0_186:
	mov.b64 	 %rd57, %fd213;
	setp.gt.s64	%p167, %rd57, 9007199254740991;
	setp.gtu.f64	%p168, %fd213, 0d001FFFFFFFFFFFFF;
	or.pred  	%p169, %p168, %p167;
	mul.f64 	%fd1362, %fd212, 0d3CF0000000000000;
	setp.lt.f64	%p170, %fd211, %fd1362;
	and.pred  	%p171, %p169, %p170;
	mul.f64 	%fd1363, %fd213, 0d3CF0000000000000;
	setp.lt.f64	%p172, %fd211, %fd1363;
	and.pred  	%p173, %p172, %p171;
	@%p173 bra 	BB0_188;
	bra.uni 	BB0_187;

BB0_185:
	mul.f64 	%fd1359, %fd212, 0d3CF0000000000000;
	setp.geu.f64	%p164, %fd211, %fd1359;
	mul.f64 	%fd1360, %fd213, 0d3CF0000000000000;
	setp.geu.f64	%p165, %fd211, %fd1360;
	or.pred  	%p166, %p164, %p165;
	@%p166 bra 	BB0_187;
	bra.uni 	BB0_188;

BB0_187:
	add.f64 	%fd1474, %fd147, %fd1473;

BB0_188:
	setp.lt.f64	%p174, %fd1452, 0d0000000000000000;
	setp.lt.f64	%p175, %fd1474, 0d0000000000000000;
	and.pred  	%p176, %p175, %p174;
	@%p176 bra 	BB0_190;

	setp.leu.f64	%p177, %fd1474, 0d0000000000000000;
	setp.leu.f64	%p178, %fd1452, 0d0000000000000000;
	or.pred  	%p179, %p177, %p178;
	@%p179 bra 	BB0_199;

BB0_190:
	setp.eq.f64	%p180, %fd1474, %fd1452;
	mov.f64 	%fd1475, 0d0000000000000000;
	@%p180 bra 	BB0_200;

	setp.eq.f64	%p181, %fd1474, 0d0000000000000000;
	setp.eq.f64	%p182, %fd1452, 0d0000000000000000;
	or.pred  	%p183, %p181, %p182;
	@%p183 bra 	BB0_199;

	sub.f64 	%fd1365, %fd1474, %fd1452;
	abs.f64 	%fd216, %fd1365;
	abs.f64 	%fd1366, %fd216;
	setp.geu.f64	%p184, %fd1366, 0d7FF0000000000000;
	@%p184 bra 	BB0_199;

	abs.f64 	%fd217, %fd1474;
	mul.f64 	%fd1367, %fd217, 0d3D30000000000000;
	setp.gt.f64	%p185, %fd216, %fd1367;
	@%p185 bra 	BB0_199;

	abs.f64 	%fd218, %fd1452;
	mul.f64 	%fd1368, %fd218, 0d3D30000000000000;
	setp.gt.f64	%p186, %fd216, %fd1368;
	@%p186 bra 	BB0_199;

	mov.b64 	 %rd58, %fd216;
	setp.gt.s64	%p187, %rd58, 9007199254740991;
	setp.gtu.f64	%p188, %fd216, 0d001FFFFFFFFFFFFF;
	or.pred  	%p189, %p187, %p188;
	@%p189 bra 	BB0_197;

	mov.b64 	 %rd59, %fd217;
	setp.lt.s64	%p190, %rd59, 9007199254740992;
	setp.le.f64	%p191, %fd217, 0d001FFFFFFFFFFFFF;
	and.pred  	%p192, %p190, %p191;
	@%p192 bra 	BB0_198;
	bra.uni 	BB0_197;

BB0_198:
	mov.b64 	 %rd60, %fd218;
	setp.gt.s64	%p196, %rd60, 9007199254740991;
	setp.gtu.f64	%p197, %fd218, 0d001FFFFFFFFFFFFF;
	or.pred  	%p198, %p197, %p196;
	mul.f64 	%fd1373, %fd217, 0d3CF0000000000000;
	setp.lt.f64	%p199, %fd216, %fd1373;
	and.pred  	%p200, %p198, %p199;
	mul.f64 	%fd1374, %fd218, 0d3CF0000000000000;
	setp.lt.f64	%p201, %fd216, %fd1374;
	and.pred  	%p202, %p201, %p200;
	@%p202 bra 	BB0_200;
	bra.uni 	BB0_199;

BB0_197:
	mul.f64 	%fd1370, %fd217, 0d3CF0000000000000;
	setp.geu.f64	%p193, %fd216, %fd1370;
	mul.f64 	%fd1371, %fd218, 0d3CF0000000000000;
	setp.geu.f64	%p194, %fd216, %fd1371;
	or.pred  	%p195, %p193, %p194;
	@%p195 bra 	BB0_199;
	bra.uni 	BB0_200;

BB0_199:
	sub.f64 	%fd1475, %fd1474, %fd1452;

BB0_200:
	setp.gt.f64	%p203, %fd105, 0d0000000000000000;
	setp.lt.f64	%p204, %fd1475, 0d0000000000000000;
	and.pred  	%p205, %p204, %p203;
	@%p205 bra 	BB0_202;

	setp.geu.f64	%p206, %fd105, 0d0000000000000000;
	setp.leu.f64	%p207, %fd1475, 0d0000000000000000;
	or.pred  	%p208, %p206, %p207;
	@%p208 bra 	BB0_211;

BB0_202:
	neg.f64 	%fd221, %fd105;
	setp.eq.f64	%p209, %fd1475, %fd221;
	mov.f64 	%fd1476, 0d0000000000000000;
	@%p209 bra 	BB0_212;

	setp.eq.f64	%p210, %fd1475, 0d0000000000000000;
	setp.eq.f64	%p211, %fd105, 0d8000000000000000;
	or.pred  	%p212, %p210, %p211;
	@%p212 bra 	BB0_211;

	add.f64 	%fd1376, %fd105, %fd1475;
	abs.f64 	%fd222, %fd1376;
	abs.f64 	%fd1377, %fd222;
	setp.geu.f64	%p213, %fd1377, 0d7FF0000000000000;
	@%p213 bra 	BB0_211;

	abs.f64 	%fd223, %fd1475;
	mul.f64 	%fd1378, %fd223, 0d3D30000000000000;
	setp.gt.f64	%p214, %fd222, %fd1378;
	@%p214 bra 	BB0_211;

	abs.f64 	%fd224, %fd221;
	mul.f64 	%fd1379, %fd224, 0d3D30000000000000;
	setp.gt.f64	%p215, %fd222, %fd1379;
	@%p215 bra 	BB0_211;

	mov.b64 	 %rd61, %fd222;
	setp.gt.s64	%p216, %rd61, 9007199254740991;
	setp.gtu.f64	%p217, %fd222, 0d001FFFFFFFFFFFFF;
	or.pred  	%p218, %p216, %p217;
	@%p218 bra 	BB0_209;

	mov.b64 	 %rd62, %fd223;
	setp.lt.s64	%p219, %rd62, 9007199254740992;
	setp.le.f64	%p220, %fd223, 0d001FFFFFFFFFFFFF;
	and.pred  	%p221, %p219, %p220;
	@%p221 bra 	BB0_210;
	bra.uni 	BB0_209;

BB0_210:
	mov.b64 	 %rd63, %fd224;
	setp.gt.s64	%p225, %rd63, 9007199254740991;
	setp.gtu.f64	%p226, %fd224, 0d001FFFFFFFFFFFFF;
	or.pred  	%p227, %p226, %p225;
	mul.f64 	%fd1384, %fd223, 0d3CF0000000000000;
	setp.lt.f64	%p228, %fd222, %fd1384;
	and.pred  	%p229, %p227, %p228;
	mul.f64 	%fd1385, %fd224, 0d3CF0000000000000;
	setp.lt.f64	%p230, %fd222, %fd1385;
	and.pred  	%p231, %p230, %p229;
	@%p231 bra 	BB0_212;
	bra.uni 	BB0_211;

BB0_209:
	mul.f64 	%fd1381, %fd223, 0d3CF0000000000000;
	setp.geu.f64	%p222, %fd222, %fd1381;
	mul.f64 	%fd1382, %fd224, 0d3CF0000000000000;
	setp.geu.f64	%p223, %fd222, %fd1382;
	or.pred  	%p224, %p222, %p223;
	@%p224 bra 	BB0_211;
	bra.uni 	BB0_212;

BB0_211:
	add.f64 	%fd1476, %fd105, %fd1475;

BB0_212:
	setp.gt.f64	%p232, %fd72, 0d0000000000000000;
	setp.lt.f64	%p233, %fd1476, 0d0000000000000000;
	and.pred  	%p234, %p233, %p232;
	@%p234 bra 	BB0_214;

	setp.geu.f64	%p235, %fd72, 0d0000000000000000;
	setp.leu.f64	%p236, %fd1476, 0d0000000000000000;
	or.pred  	%p237, %p235, %p236;
	@%p237 bra 	BB0_223;

BB0_214:
	neg.f64 	%fd227, %fd72;
	setp.eq.f64	%p238, %fd1476, %fd227;
	mov.f64 	%fd1477, 0d0000000000000000;
	@%p238 bra 	BB0_224;

	setp.eq.f64	%p239, %fd1476, 0d0000000000000000;
	setp.eq.f64	%p240, %fd72, 0d8000000000000000;
	or.pred  	%p241, %p239, %p240;
	@%p241 bra 	BB0_223;

	add.f64 	%fd1387, %fd72, %fd1476;
	abs.f64 	%fd228, %fd1387;
	abs.f64 	%fd1388, %fd228;
	setp.geu.f64	%p242, %fd1388, 0d7FF0000000000000;
	@%p242 bra 	BB0_223;

	abs.f64 	%fd229, %fd1476;
	mul.f64 	%fd1389, %fd229, 0d3D30000000000000;
	setp.gt.f64	%p243, %fd228, %fd1389;
	@%p243 bra 	BB0_223;

	abs.f64 	%fd230, %fd227;
	mul.f64 	%fd1390, %fd230, 0d3D30000000000000;
	setp.gt.f64	%p244, %fd228, %fd1390;
	@%p244 bra 	BB0_223;

	mov.b64 	 %rd64, %fd228;
	setp.gt.s64	%p245, %rd64, 9007199254740991;
	setp.gtu.f64	%p246, %fd228, 0d001FFFFFFFFFFFFF;
	or.pred  	%p247, %p245, %p246;
	@%p247 bra 	BB0_221;

	mov.b64 	 %rd65, %fd229;
	setp.lt.s64	%p248, %rd65, 9007199254740992;
	setp.le.f64	%p249, %fd229, 0d001FFFFFFFFFFFFF;
	and.pred  	%p250, %p248, %p249;
	@%p250 bra 	BB0_222;
	bra.uni 	BB0_221;

BB0_222:
	mov.b64 	 %rd66, %fd230;
	setp.gt.s64	%p254, %rd66, 9007199254740991;
	setp.gtu.f64	%p255, %fd230, 0d001FFFFFFFFFFFFF;
	or.pred  	%p256, %p255, %p254;
	mul.f64 	%fd1395, %fd229, 0d3CF0000000000000;
	setp.lt.f64	%p257, %fd228, %fd1395;
	and.pred  	%p258, %p256, %p257;
	mul.f64 	%fd1396, %fd230, 0d3CF0000000000000;
	setp.lt.f64	%p259, %fd228, %fd1396;
	and.pred  	%p260, %p259, %p258;
	@%p260 bra 	BB0_224;
	bra.uni 	BB0_223;

BB0_221:
	mul.f64 	%fd1392, %fd229, 0d3CF0000000000000;
	setp.geu.f64	%p251, %fd228, %fd1392;
	mul.f64 	%fd1393, %fd230, 0d3CF0000000000000;
	setp.geu.f64	%p252, %fd228, %fd1393;
	or.pred  	%p253, %p251, %p252;
	@%p253 bra 	BB0_223;
	bra.uni 	BB0_224;

BB0_223:
	add.f64 	%fd1477, %fd72, %fd1476;

BB0_224:
	ld.param.u64 	%rd71, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0];
	add.s64 	%rd68, %rd71, %rd45;
	st.global.f64 	[%rd68], %fd1477;
	ret;
}


  